{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Trading Agent v1.0\n",
    "\n",
    "**A reinforcement learning agent that learns to trade Gold by doing.**\n",
    "\n",
    "- Starts with $1,000\n",
    "- Can go LONG or SHORT any size (1+ units)\n",
    "- Can pyramid, scale in/out, set stops\n",
    "- Gets liquidated at 30% equity ($300)\n",
    "- Learns from blowups and successes\n",
    "- Goal: MAXIMIZE PROFIT\n",
    "\n",
    "No rules. Just profit."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Setup\n",
    "!pip install xgboost scikit-learn pandas numpy matplotlib seaborn -q\n",
    "\n",
    "import os\n",
    "if os.path.exists('gold-ml-trading'):\n",
    "    %cd gold-ml-trading\n",
    "    !git pull\n",
    "else:\n",
    "    !git clone https://github.com/altommo/gold-ml-trading.git\n",
    "    %cd gold-ml-trading"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For the neural network\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "print(\"Libraries loaded!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================\n# TRADING ENVIRONMENT\n# - Reward = Points (% account change)\n# - Losses hurt 3x\n# - Winning trades get +2 bonus\n# - Drawdown penalty when equity drops from peak\n# - Consistency bonus (rolling Sharpe)\n# - Liquidation penalty scales by opportunity lost\n# ============================================================\n\nclass TradingEnvironment:\n    def __init__(self, df, initial_capital=1000, margin_per_unit=36.57,\n                 unit_size=0.15, spread_pct=0.0004, liquidation_pct=0.30,\n                 max_bars=5000):\n        \"\"\"\n        Real broker simulation with robust reward system.\n        \"\"\"\n        self.df = df.reset_index(drop=True)\n        self.initial_capital = initial_capital\n        self.margin_per_unit = margin_per_unit\n        self.unit_size = unit_size\n        self.spread_pct = spread_pct\n        self.liquidation_level = initial_capital * liquidation_pct\n        self.max_bars = max_bars\n        \n        # Reward parameters\n        self.loss_multiplier = 3.0  # Losses hurt 3x\n        self.win_bonus = 2.0  # Bonus points for profitable trades\n        self.drawdown_penalty_threshold = 0.05  # Start penalizing at 5% drawdown\n        self.drawdown_penalty_rate = 0.5  # Penalty per % drawdown above threshold\n        self.sharpe_window = 100  # Bars for rolling Sharpe\n        self.sharpe_bonus_interval = 100  # Give bonus every N bars\n        \n        self.reset()\n    \n    def reset(self, start_idx=None):\n        \"\"\"Reset environment for new episode.\"\"\"\n        self.capital = self.initial_capital\n        self.position = 0\n        self.entry_price = 0\n        self.entry_idx = 0\n        self.entry_indicators = {}\n        self.stop_loss = None\n        self.trailing_stop = None\n        self.trailing_stop_pct = None\n        self.highest_since_entry = 0\n        self.lowest_since_entry = float('inf')\n        \n        self.trades = []  # Detailed trade log\n        self.equity_curve = [self.initial_capital]\n        self.bars_since_trade = 0\n        \n        # For drawdown tracking\n        self.peak_equity = self.initial_capital\n        self.current_drawdown = 0\n        \n        # For rolling Sharpe\n        self.bar_returns = []  # % returns per bar\n        self.last_equity = self.initial_capital\n        \n        if start_idx is None:\n            self.current_idx = random.randint(200, len(self.df) - self.max_bars - 100)\n        else:\n            self.current_idx = start_idx\n        \n        self.start_idx = self.current_idx\n        self.done = False\n        \n        return self._get_state()\n    \n    def _get_price(self):\n        return self.df.iloc[self.current_idx]['close']\n    \n    def _get_indicators(self):\n        \"\"\"Get current indicator values for logging.\"\"\"\n        row = self.df.iloc[self.current_idx]\n        return {\n            'idx': self.current_idx,\n            'price': row['close'],\n            'wt1': row.get('wt1', 0),\n            'wt2': row.get('wt2', 0),\n            'rsi': row.get('rsi', 50),\n            'macd_hist': row.get('macd_hist', 0),\n            'bb_pct': row.get('bb_pct', 0.5),\n            'trend_score': row.get('trend_score', 0),\n            'atr_pct': row.get('atr_pct', 0),\n            'hour': row.get('hour', 12),\n        }\n    \n    def _get_state(self):\n        \"\"\"Get current state for agent.\"\"\"\n        row = self.df.iloc[self.current_idx]\n        price = row['close']\n        \n        if self.position != 0:\n            if self.position > 0:\n                unrealized_pnl = (price - self.entry_price) / self.entry_price\n            else:\n                unrealized_pnl = (self.entry_price - price) / self.entry_price\n            unrealized_pnl *= abs(self.position)\n        else:\n            unrealized_pnl = 0\n        \n        margin_used = abs(self.position) * self.margin_per_unit\n        margin_pct = margin_used / self.capital if self.capital > 0 else 1\n        available_margin = self.capital - margin_used\n        max_units = int(available_margin / self.margin_per_unit)\n        \n        state = {\n            'wt1': row.get('wt1', 0),\n            'wt2': row.get('wt2', 0),\n            'wolfpack': row.get('wolfpack', 0),\n            'rsi': row.get('rsi', 50),\n            'stoch_rsi': row.get('stoch_rsi', 50),\n            'macd_hist': row.get('macd_hist', 0),\n            'bb_pct': row.get('bb_pct', 0.5),\n            'atr_pct': row.get('atr_pct', 0),\n            'roc_5': row.get('roc_5', 0),\n            'roc_10': row.get('roc_10', 0),\n            'price_vs_ma20': row.get('price_vs_ma20', 0),\n            'price_vs_ma50': row.get('price_vs_ma50', 0),\n            'trend_score': row.get('trend_score', 0),\n            'volatility_24h': row.get('volatility_24h', 0),\n            'hour': row.get('hour', 12),\n            'day_of_week': row.get('day_of_week', 2),\n            'position': self.position,\n            'position_side': 1 if self.position > 0 else (-1 if self.position < 0 else 0),\n            'unrealized_pnl': unrealized_pnl,\n            'margin_pct': margin_pct,\n            'max_units': max_units,\n            'capital': self.capital,\n            'capital_pct': self.capital / self.initial_capital,\n            'bars_since_trade': min(self.bars_since_trade / 168, 1),\n            'drawdown_pct': self.current_drawdown,  # Add drawdown to state\n        }\n        \n        return state\n    \n    def _calculate_equity(self):\n        if self.position == 0:\n            return self.capital\n        \n        price = self._get_price()\n        notional = abs(self.position) * self.unit_size * price\n        \n        if self.position > 0:\n            pnl = (price - self.entry_price) / self.entry_price * notional\n        else:\n            pnl = (self.entry_price - price) / self.entry_price * notional\n        \n        return self.capital + pnl\n    \n    def _pnl_to_points(self, pnl_dollars, capital_before, is_trade_close=False):\n        \"\"\"Convert P&L to points. Losses hurt 3x. Wins get bonus.\"\"\"\n        if capital_before <= 0:\n            return 0\n        \n        pnl_pct = pnl_dollars / capital_before * 100\n        \n        if pnl_pct < 0:\n            return pnl_pct * self.loss_multiplier  # Losses hurt 3x\n        else:\n            bonus = self.win_bonus if is_trade_close and pnl_pct > 0 else 0\n            return pnl_pct + bonus  # Wins get +2 bonus\n    \n    def _calculate_drawdown_penalty(self, equity):\n        \"\"\"Penalize when in drawdown above threshold.\"\"\"\n        if equity > self.peak_equity:\n            self.peak_equity = equity\n            self.current_drawdown = 0\n            return 0\n        \n        self.current_drawdown = (self.peak_equity - equity) / self.peak_equity\n        \n        if self.current_drawdown > self.drawdown_penalty_threshold:\n            excess_dd = (self.current_drawdown - self.drawdown_penalty_threshold) * 100\n            return -excess_dd * self.drawdown_penalty_rate\n        \n        return 0\n    \n    def _calculate_sharpe_bonus(self):\n        \"\"\"Bonus for consistent returns (rolling Sharpe).\"\"\"\n        if len(self.bar_returns) < self.sharpe_window:\n            return 0\n        \n        recent_returns = self.bar_returns[-self.sharpe_window:]\n        mean_ret = np.mean(recent_returns)\n        std_ret = np.std(recent_returns)\n        \n        if std_ret < 0.001:  # Avoid division by zero\n            if mean_ret > 0:\n                sharpe = 2.0  # Cap at 2 for very consistent positive returns\n            else:\n                sharpe = -2.0\n        else:\n            sharpe = mean_ret / std_ret\n        \n        # Reward positive Sharpe, penalize negative\n        # Scale: Sharpe of 1.0 = +5 points, Sharpe of -1.0 = -5 points\n        return sharpe * 5\n    \n    def step(self, action):\n        if self.done:\n            return self._get_state(), 0, True, {}\n        \n        price = self._get_price()\n        reward = 0\n        info = {'trade': None}\n        capital_before = self.capital\n        bars_into_episode = self.current_idx - self.start_idx\n        \n        # Update trailing stop\n        if self.position > 0:\n            self.highest_since_entry = max(self.highest_since_entry, price)\n            if self.trailing_stop_pct:\n                self.trailing_stop = self.highest_since_entry * (1 - self.trailing_stop_pct)\n        elif self.position < 0:\n            self.lowest_since_entry = min(self.lowest_since_entry, price)\n            if self.trailing_stop_pct:\n                self.trailing_stop = self.lowest_since_entry * (1 + self.trailing_stop_pct)\n        \n        # Check stops\n        stop_triggered = False\n        exit_reason = None\n        if self.position > 0:\n            low = self.df.iloc[self.current_idx]['low']\n            if self.stop_loss and low <= self.stop_loss:\n                stop_triggered = True\n                exit_price = self.stop_loss\n                exit_reason = 'stop_loss'\n            elif self.trailing_stop and low <= self.trailing_stop:\n                stop_triggered = True\n                exit_price = self.trailing_stop\n                exit_reason = 'trailing_stop'\n        elif self.position < 0:\n            high = self.df.iloc[self.current_idx]['high']\n            if self.stop_loss and high >= self.stop_loss:\n                stop_triggered = True\n                exit_price = self.stop_loss\n                exit_reason = 'stop_loss'\n            elif self.trailing_stop and high >= self.trailing_stop:\n                stop_triggered = True\n                exit_price = self.trailing_stop\n                exit_reason = 'trailing_stop'\n        \n        if stop_triggered:\n            pnl = self._close_position(exit_price, exit_reason)\n            reward += self._pnl_to_points(pnl, capital_before, is_trade_close=True)\n            capital_before = self.capital\n            info['trade'] = exit_reason\n        \n        # Process action\n        action_type = action.get('type', 'HOLD')\n        units = action.get('units', 0)\n        \n        if action_type == 'LONG' and units > 0:\n            cost = self._open_long(units, price)\n            reward += self._pnl_to_points(cost, capital_before)\n            capital_before = self.capital\n            if action.get('stop_loss'):\n                self.stop_loss = price * (1 - action['stop_loss'])\n            if action.get('trailing_stop'):\n                self.trailing_stop_pct = action['trailing_stop']\n                self.highest_since_entry = price\n            self.bars_since_trade = 0\n            info['trade'] = f'LONG {units}'\n            \n        elif action_type == 'SHORT' and units > 0:\n            cost = self._open_short(units, price)\n            reward += self._pnl_to_points(cost, capital_before)\n            capital_before = self.capital\n            if action.get('stop_loss'):\n                self.stop_loss = price * (1 + action['stop_loss'])\n            if action.get('trailing_stop'):\n                self.trailing_stop_pct = action['trailing_stop']\n                self.lowest_since_entry = price\n            self.bars_since_trade = 0\n            info['trade'] = f'SHORT {units}'\n            \n        elif action_type == 'CLOSE' and self.position != 0:\n            pnl = self._close_position(price, 'manual_close')\n            reward += self._pnl_to_points(pnl, capital_before, is_trade_close=True)\n            self.bars_since_trade = 0\n            info['trade'] = 'CLOSE'\n            \n        else:\n            self.bars_since_trade += 1\n            if self.bars_since_trade > 168 and self.position == 0:\n                reward -= 0.1\n        \n        # Update equity and track bar return\n        equity = self._calculate_equity()\n        bar_return = (equity - self.last_equity) / self.last_equity * 100 if self.last_equity > 0 else 0\n        self.bar_returns.append(bar_return)\n        self.last_equity = equity\n        self.equity_curve.append(equity)\n        \n        # Drawdown penalty\n        dd_penalty = self._calculate_drawdown_penalty(equity)\n        reward += dd_penalty\n        \n        # Sharpe bonus every N bars\n        if bars_into_episode > 0 and bars_into_episode % self.sharpe_bonus_interval == 0:\n            sharpe_bonus = self._calculate_sharpe_bonus()\n            reward += sharpe_bonus\n            info['sharpe_bonus'] = sharpe_bonus\n        \n        # Check liquidation\n        if equity <= self.liquidation_level:\n            self.done = True\n            # Penalty scales by opportunity lost\n            bars_remaining = self.max_bars - bars_into_episode\n            opportunity_lost = bars_remaining / self.max_bars * 100\n            reward -= (50 + opportunity_lost)  # -50 base + opportunity cost\n            info['liquidated'] = True\n            if self.position != 0:\n                self._close_position(price, 'liquidation')\n        \n        # Move to next bar\n        self.current_idx += 1\n        if self.current_idx >= len(self.df) - 1 or bars_into_episode >= self.max_bars:\n            self.done = True\n            if self.position != 0:\n                pnl = self._close_position(self._get_price(), 'end_of_episode')\n                reward += self._pnl_to_points(pnl, self.capital - pnl, is_trade_close=True)\n            \n            # Final Sharpe bonus at end\n            final_sharpe = self._calculate_sharpe_bonus()\n            reward += final_sharpe * 2  # Double weight for final Sharpe\n        \n        return self._get_state(), reward, self.done, info\n    \n    def _open_long(self, units, price):\n        margin_needed = units * self.margin_per_unit\n        margin_used = abs(self.position) * self.margin_per_unit\n        available = self.capital - margin_used\n        \n        if margin_needed > available:\n            units = int(available / self.margin_per_unit)\n            if units <= 0:\n                return 0\n        \n        spread_cost = units * self.unit_size * price * self.spread_pct\n        self.capital -= spread_cost\n        \n        if self.position < 0:\n            pnl = self._close_position(price, 'reversed')\n            self.position = units\n            self.entry_price = price\n            self.entry_idx = self.current_idx\n            self.entry_indicators = self._get_indicators()\n            return pnl - spread_cost\n        \n        if self.position > 0:\n            total_cost = self.position * self.entry_price + units * price\n            self.position += units\n            self.entry_price = total_cost / self.position\n        else:\n            self.position = units\n            self.entry_price = price\n            self.entry_idx = self.current_idx\n            self.entry_indicators = self._get_indicators()\n            self.highest_since_entry = price\n        \n        return -spread_cost\n    \n    def _open_short(self, units, price):\n        margin_needed = units * self.margin_per_unit\n        margin_used = abs(self.position) * self.margin_per_unit\n        available = self.capital - margin_used\n        \n        if margin_needed > available:\n            units = int(available / self.margin_per_unit)\n            if units <= 0:\n                return 0\n        \n        spread_cost = units * self.unit_size * price * self.spread_pct\n        self.capital -= spread_cost\n        \n        if self.position > 0:\n            pnl = self._close_position(price, 'reversed')\n            self.position = -units\n            self.entry_price = price\n            self.entry_idx = self.current_idx\n            self.entry_indicators = self._get_indicators()\n            return pnl - spread_cost\n        \n        if self.position < 0:\n            total_cost = abs(self.position) * self.entry_price + units * price\n            self.position -= units\n            self.entry_price = total_cost / abs(self.position)\n        else:\n            self.position = -units\n            self.entry_price = price\n            self.entry_idx = self.current_idx\n            self.entry_indicators = self._get_indicators()\n            self.lowest_since_entry = price\n        \n        return -spread_cost\n    \n    def _close_position(self, price, exit_reason='unknown'):\n        \"\"\"Close position with detailed logging.\"\"\"\n        if self.position == 0:\n            return 0\n        \n        notional = abs(self.position) * self.unit_size * price\n        \n        if self.position > 0:\n            pnl_pct = (price - self.entry_price) / self.entry_price\n        else:\n            pnl_pct = (self.entry_price - price) / self.entry_price\n        \n        pnl_dollars = pnl_pct * notional\n        spread_cost = notional * self.spread_pct\n        pnl_dollars -= spread_cost\n        \n        # Calculate account impact\n        account_pct_change = pnl_dollars / self.capital * 100 if self.capital > 0 else 0\n        \n        self.capital += pnl_dollars\n        \n        # Detailed trade log\n        exit_indicators = self._get_indicators()\n        self.trades.append({\n            'entry_idx': self.entry_idx,\n            'exit_idx': self.current_idx,\n            'bars_held': self.current_idx - self.entry_idx,\n            'position': self.position,\n            'entry_price': self.entry_price,\n            'exit_price': price,\n            'pnl_dollars': pnl_dollars,\n            'pnl_pct': pnl_pct * 100,\n            'account_pct_change': account_pct_change,\n            'exit_reason': exit_reason,\n            'entry_indicators': self.entry_indicators.copy(),\n            'exit_indicators': exit_indicators,\n        })\n        \n        self.position = 0\n        self.entry_price = 0\n        self.entry_idx = 0\n        self.entry_indicators = {}\n        self.stop_loss = None\n        self.trailing_stop = None\n        self.trailing_stop_pct = None\n        \n        return pnl_dollars\n    \n    def get_episode_stats(self):\n        \"\"\"Get episode statistics.\"\"\"\n        trades_df = pd.DataFrame(self.trades) if self.trades else pd.DataFrame()\n        \n        # Calculate episode Sharpe\n        if len(self.bar_returns) > 10:\n            mean_ret = np.mean(self.bar_returns)\n            std_ret = np.std(self.bar_returns)\n            episode_sharpe = mean_ret / std_ret if std_ret > 0.001 else 0\n        else:\n            episode_sharpe = 0\n        \n        stats = {\n            'starting_capital': self.initial_capital,\n            'ending_capital': self.capital,\n            'total_return_pct': (self.capital - self.initial_capital) / self.initial_capital * 100,\n            'num_trades': len(self.trades),\n            'bars_traded': self.current_idx - self.start_idx,\n            'liquidated': self.capital <= self.liquidation_level,\n            'max_drawdown': self.current_drawdown * 100 if hasattr(self, 'current_drawdown') else 0,\n            'episode_sharpe': episode_sharpe,\n            'trades': self.trades,  # Full trade log for analysis\n        }\n        \n        if len(trades_df) > 0:\n            stats['win_rate'] = (trades_df['pnl_dollars'] > 0).mean() * 100\n            stats['avg_win'] = trades_df[trades_df['pnl_dollars'] > 0]['pnl_dollars'].mean() if (trades_df['pnl_dollars'] > 0).any() else 0\n            stats['avg_loss'] = trades_df[trades_df['pnl_dollars'] < 0]['pnl_dollars'].mean() if (trades_df['pnl_dollars'] < 0).any() else 0\n            stats['best_trade'] = trades_df['pnl_dollars'].max()\n            stats['worst_trade'] = trades_df['pnl_dollars'].min()\n            stats['avg_bars_held'] = trades_df['bars_held'].mean()\n            \n            # Outliers\n            stats['big_wins'] = trades_df[trades_df['account_pct_change'] > 5].to_dict('records')\n            stats['big_losses'] = trades_df[trades_df['account_pct_change'] < -3].to_dict('records')\n        \n        # Calculate max drawdown from equity curve\n        equity = np.array(self.equity_curve)\n        rolling_max = np.maximum.accumulate(equity)\n        drawdown = (rolling_max - equity) / rolling_max * 100\n        stats['max_drawdown'] = drawdown.max()\n        \n        return stats\n\nprint(\"TradingEnvironment defined!\")\nprint(\"  - Losses hurt 3x\")\nprint(\"  - Winning trades get +2 bonus\")\nprint(\"  - Drawdown penalty above 5%\")\nprint(\"  - Rolling Sharpe bonus every 100 bars\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================\n# RL TRADING AGENT\n# - Epsilon decays only on NEW BEST\n# - Trains during episodes + full episode replay at end\n# ============================================================\n\nclass TradingAgent:\n    def __init__(self, state_size=25, learning_rate=0.001, gamma=0.95,\n                 epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.95):\n        \"\"\"\n        RL Agent that learns to trade.\n        Trains during episodes + episode replay at end.\n        \"\"\"\n        self.state_size = state_size\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        self.learning_rate = learning_rate\n        \n        # Experience replay buffer (all experiences)\n        self.memory = deque(maxlen=100000)\n        \n        # Current episode experiences (for end-of-episode training)\n        self.episode_memory = []\n        \n        # Action space\n        self.action_size = 28\n        \n        # Q-network\n        self.model = MLPRegressor(\n            hidden_layer_sizes=(128, 64, 32),\n            activation='relu',\n            solver='adam',\n            learning_rate_init=learning_rate,\n            max_iter=1,\n            warm_start=True\n        )\n        \n        # Initialize model\n        dummy_X = np.random.randn(100, state_size)\n        dummy_y = np.random.randn(100, self.action_size)\n        self.model.fit(dummy_X, dummy_y)\n        \n        self.scaler = StandardScaler()\n        self.scaler_fitted = False\n    \n    def state_to_array(self, state):\n        keys = ['wt1', 'wt2', 'wolfpack', 'rsi', 'stoch_rsi', 'macd_hist',\n                'bb_pct', 'atr_pct', 'roc_5', 'roc_10', 'price_vs_ma20',\n                'price_vs_ma50', 'trend_score', 'volatility_24h', 'hour',\n                'day_of_week', 'position', 'position_side', 'unrealized_pnl',\n                'margin_pct', 'max_units', 'capital', 'capital_pct', \n                'bars_since_trade', 'drawdown_pct']  # Added drawdown_pct\n        \n        arr = np.array([state.get(k, 0) for k in keys], dtype=np.float32)\n        arr = np.nan_to_num(arr, nan=0, posinf=0, neginf=0)\n        return arr\n    \n    def action_to_dict(self, action_idx, max_units):\n        if action_idx == 0:\n            return {'type': 'HOLD'}\n        elif 1 <= action_idx <= 10:\n            units = min(action_idx, max_units)\n            return {'type': 'LONG', 'units': max(1, units)}\n        elif 11 <= action_idx <= 20:\n            units = min(action_idx - 10, max_units)\n            return {'type': 'SHORT', 'units': max(1, units)}\n        elif action_idx == 21:\n            return {'type': 'CLOSE'}\n        elif action_idx == 22:\n            return {'type': 'LONG', 'units': min(3, max_units), 'stop_loss': 0.01}\n        elif action_idx == 23:\n            return {'type': 'LONG', 'units': min(5, max_units), 'stop_loss': 0.02}\n        elif action_idx == 24:\n            return {'type': 'LONG', 'units': min(3, max_units), 'trailing_stop': 0.02}\n        elif action_idx == 25:\n            return {'type': 'SHORT', 'units': min(3, max_units), 'stop_loss': 0.01}\n        elif action_idx == 26:\n            return {'type': 'SHORT', 'units': min(5, max_units), 'stop_loss': 0.02}\n        elif action_idx == 27:\n            return {'type': 'SHORT', 'units': min(3, max_units), 'trailing_stop': 0.02}\n        else:\n            return {'type': 'HOLD'}\n    \n    def choose_action(self, state, training=True):\n        if training and random.random() < self.epsilon:\n            return random.randint(0, self.action_size - 1)\n        \n        state_arr = self.state_to_array(state).reshape(1, -1)\n        if self.scaler_fitted:\n            state_arr = self.scaler.transform(state_arr)\n        q_values = self.model.predict(state_arr)\n        return np.argmax(q_values[0])\n    \n    def remember(self, state, action, reward, next_state, done):\n        \"\"\"Store in both global and episode memory.\"\"\"\n        experience = (state, action, reward, next_state, done)\n        self.memory.append(experience)\n        self.episode_memory.append(experience)\n    \n    def start_episode(self):\n        \"\"\"Clear episode memory for new episode.\"\"\"\n        self.episode_memory = []\n    \n    def replay(self, batch_size=64):\n        \"\"\"Train on random batch from all experiences.\"\"\"\n        if len(self.memory) < batch_size:\n            return 0\n        \n        batch = random.sample(self.memory, batch_size)\n        return self._train_on_batch(batch)\n    \n    def replay_episode(self, boost_factor=1.0):\n        \"\"\"\n        Train on the full episode that just completed.\n        boost_factor: multiply rewards (e.g., 2.0 for good episodes, 0.5 for bad)\n        \"\"\"\n        if len(self.episode_memory) < 10:\n            return 0\n        \n        # Train on episode experiences multiple times for reinforcement\n        batch = self.episode_memory.copy()\n        \n        # Apply boost factor to rewards\n        if boost_factor != 1.0:\n            batch = [(s, a, r * boost_factor, ns, d) for s, a, r, ns, d in batch]\n        \n        return self._train_on_batch(batch)\n    \n    def _train_on_batch(self, batch):\n        \"\"\"Internal training on a batch of experiences.\"\"\"\n        if len(batch) == 0:\n            return 0\n        \n        states = np.array([self.state_to_array(s) for s, _, _, _, _ in batch])\n        next_states = np.array([self.state_to_array(ns) for _, _, _, ns, _ in batch])\n        \n        if not self.scaler_fitted:\n            self.scaler.fit(states)\n            self.scaler_fitted = True\n        \n        states = self.scaler.transform(states)\n        next_states = self.scaler.transform(next_states)\n        \n        current_q = self.model.predict(states)\n        next_q = self.model.predict(next_states)\n        \n        for i, (state, action, reward, next_state, done) in enumerate(batch):\n            if done:\n                target = reward\n            else:\n                target = reward + self.gamma * np.max(next_q[i])\n            current_q[i][action] = target\n        \n        self.model.fit(states, current_q)\n        return np.mean(np.abs(current_q))\n    \n    def decay_epsilon(self):\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\nprint(\"TradingAgent defined!\")\nprint(\"  - State size: 25 features (includes drawdown_pct)\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# INDICATOR FUNCTIONS (same as before)\n",
    "# ============================================================\n",
    "\n",
    "def calculate_wavetrend(df, n1=10, n2=21):\n",
    "    df = df.copy()\n",
    "    ap = (df['high'] + df['low'] + df['close']) / 3\n",
    "    esa = ap.ewm(span=n1, adjust=False).mean()\n",
    "    d = (ap - esa).abs().ewm(span=n1, adjust=False).mean()\n",
    "    ci = (ap - esa) / (0.015 * d)\n",
    "    df['wt1'] = ci.ewm(span=n2, adjust=False).mean()\n",
    "    df['wt2'] = df['wt1'].rolling(4).mean()\n",
    "    return df\n",
    "\n",
    "def calculate_wolfpack(df):\n",
    "    df = df.copy()\n",
    "    df['wolfpack'] = df['close'].ewm(span=3, adjust=False).mean() - df['close'].ewm(span=8, adjust=False).mean()\n",
    "    return df\n",
    "\n",
    "def calculate_rsi(df, period=14):\n",
    "    df = df.copy()\n",
    "    delta = df['close'].diff()\n",
    "    gain = delta.clip(lower=0).rolling(period).mean()\n",
    "    loss = (-delta.clip(upper=0)).rolling(period).mean()\n",
    "    df['rsi'] = 100 - (100 / (1 + gain / loss))\n",
    "    return df\n",
    "\n",
    "def calculate_atr(df, period=14):\n",
    "    df = df.copy()\n",
    "    df['atr'] = (df['high'] - df['low']).rolling(period).mean()\n",
    "    df['atr_pct'] = df['atr'] / df['close'] * 100\n",
    "    return df\n",
    "\n",
    "def calculate_moving_averages(df):\n",
    "    df = df.copy()\n",
    "    df['ma20'] = df['close'].rolling(20).mean()\n",
    "    df['ma50'] = df['close'].rolling(50).mean()\n",
    "    df['price_vs_ma20'] = (df['close'] - df['ma20']) / df['ma20'] * 100\n",
    "    df['price_vs_ma50'] = (df['close'] - df['ma50']) / df['ma50'] * 100\n",
    "    return df\n",
    "\n",
    "def calculate_returns(df):\n",
    "    df = df.copy()\n",
    "    df['ret_1h'] = df['close'].pct_change() * 100\n",
    "    return df\n",
    "\n",
    "def calculate_bollinger_bands(df, period=20, std_dev=2):\n",
    "    df = df.copy()\n",
    "    df['bb_mid'] = df['close'].rolling(period).mean()\n",
    "    df['bb_std'] = df['close'].rolling(period).std()\n",
    "    df['bb_upper'] = df['bb_mid'] + (df['bb_std'] * std_dev)\n",
    "    df['bb_lower'] = df['bb_mid'] - (df['bb_std'] * std_dev)\n",
    "    df['bb_pct'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])\n",
    "    return df\n",
    "\n",
    "def calculate_momentum(df):\n",
    "    df = df.copy()\n",
    "    df['roc_5'] = (df['close'] / df['close'].shift(5) - 1) * 100\n",
    "    df['roc_10'] = (df['close'] / df['close'].shift(10) - 1) * 100\n",
    "    ema12 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    ema26 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['macd'] = ema12 - ema26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    rsi = df['rsi']\n",
    "    rsi_min = rsi.rolling(14).min()\n",
    "    rsi_max = rsi.rolling(14).max()\n",
    "    df['stoch_rsi'] = (rsi - rsi_min) / (rsi_max - rsi_min) * 100\n",
    "    return df\n",
    "\n",
    "def calculate_time_features(df):\n",
    "    df = df.copy()\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        df['hour'] = df.index.hour\n",
    "        df['day_of_week'] = df.index.dayofweek\n",
    "    return df\n",
    "\n",
    "def calculate_trend(df):\n",
    "    df = df.copy()\n",
    "    df['trend_score'] = np.where(df['ma20'] > df['ma50'], 1, -1)\n",
    "    return df\n",
    "\n",
    "def calculate_volatility(df):\n",
    "    df = df.copy()\n",
    "    df['volatility_24h'] = df['ret_1h'].rolling(24).std()\n",
    "    return df\n",
    "\n",
    "def add_all_indicators(df):\n",
    "    df = calculate_wavetrend(df)\n",
    "    df = calculate_wolfpack(df)\n",
    "    df = calculate_rsi(df)\n",
    "    df = calculate_atr(df)\n",
    "    df = calculate_moving_averages(df)\n",
    "    df = calculate_returns(df)\n",
    "    df = calculate_bollinger_bands(df)\n",
    "    df = calculate_momentum(df)\n",
    "    df = calculate_time_features(df)\n",
    "    df = calculate_trend(df)\n",
    "    df = calculate_volatility(df)\n",
    "    return df\n",
    "\n",
    "print(\"Indicators defined!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('data/XAUUSD_KAGGLE_1h.csv', parse_dates=['datetime'], index_col='datetime')\n",
    "print(f\"Loaded {len(df):,} bars\")\n",
    "print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "\n",
    "print(\"\\nCalculating indicators...\")\n",
    "df = add_all_indicators(df)\n",
    "df = df.dropna()\n",
    "print(f\"Clean data: {len(df):,} bars\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================\n# TRAINING LOOP\n# - Train during episodes (every 4 steps) + FULL episode replay at end\n# - Episode replay boosted for good results, reduced for bad\n# - Phase 1: Pure exploration (eps 1-100, epsilon=1.0)\n# - Phase 2: Learning (epsilon decays on new best until 0.1)\n# - Phase 3: Baselined (epsilon=0.1), plateau detection starts\n# - BEST = highest total POINTS (not just P&L)\n# ============================================================\n\nprint(\"=\"*70)\nprint(\"RL TRADING AGENT - TRAIN UNTIL PLATEAU\")\nprint(\"=\"*70)\n\n# Initialize\nenv = TradingEnvironment(df, initial_capital=1000, max_bars=5000)\nagent = TradingAgent(epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.95)\n\n# Training parameters\nMIN_EXPLORATION_EPISODES = 100   # Pure exploration (epsilon=1.0)\nPLATEAU_THRESHOLD = 150          # Episodes without improvement after baselined\nBATCH_SIZE = 64\nTRAIN_EVERY = 4\n\n# Tracking\nepisode_rewards = []\nepisode_stats = []\nall_trades = []\nbest_stats = None\nbest_points = -float('inf')  # Track best by POINTS, not P&L\nepisodes_since_best = 0\nplateau_counter = 0  # Only counts when baselined (epsilon=0.1)\nepisode = 0\n\nprint(f\"\\nReward System:\")\nprint(f\"  - Losses hurt 3x\")\nprint(f\"  - Winning trades get +2 bonus\")\nprint(f\"  - Drawdown penalty above 5%\")\nprint(f\"  - Rolling Sharpe bonus every 100 bars\")\nprint(f\"  - BEST = highest total POINTS (not just P&L)\")\nprint(f\"\\nPhase 1: Episodes 1-{MIN_EXPLORATION_EPISODES} = pure exploration (epsilon=1.0)\")\nprint(f\"Phase 2: Learning until epsilon reaches 0.1\")\nprint(f\"Phase 3: BASELINED (epsilon=0.1) - plateau detection ({PLATEAU_THRESHOLD} eps no improvement)\")\nprint()\nprint(f\"{'Ep':>5} | {'Points':>8} | {'Return':>8} | {'MaxDD':>6} | {'Sharpe':>6} | {'Trades':>6} | {'WR':>5} | {'Eps':>5} | {'Status':<12}\")\nprint(\"-\" * 100)\n\nwhile True:\n    # Start new episode\n    agent.start_episode()\n    state = env.reset()\n    total_reward = 0\n    step = 0\n    \n    # Run episode\n    while not env.done:\n        action_idx = agent.choose_action(state)\n        action = agent.action_to_dict(action_idx, state['max_units'])\n        next_state, reward, done, info = env.step(action)\n        \n        agent.remember(state, action_idx, reward, next_state, done)\n        \n        # Train during episode on random past experiences\n        if step % TRAIN_EVERY == 0 and len(agent.memory) >= BATCH_SIZE:\n            agent.replay(BATCH_SIZE)\n        \n        total_reward += reward\n        state = next_state\n        step += 1\n    \n    # Episode complete - get stats\n    stats = env.get_episode_stats()\n    stats['total_points'] = total_reward  # Add points to stats\n    episode_rewards.append(total_reward)\n    episode_stats.append(stats)\n    all_trades.extend(stats.get('trades', []))\n    \n    # Determine boost factor for episode replay\n    if stats.get('liquidated'):\n        boost_factor = 0.25  # Reduce learning from liquidated episodes\n    elif total_reward > 50:\n        boost_factor = 3.0   # Strong boost for high-scoring episodes\n    elif total_reward > 0:\n        boost_factor = 2.0   # Boost positive point episodes\n    elif total_reward > -20:\n        boost_factor = 0.75  # Slight reduction for small negative\n    else:\n        boost_factor = 0.5   # Reduce learning from bad episodes\n    \n    # Train on full episode with boost\n    agent.replay_episode(boost_factor=boost_factor)\n    \n    # Check if baselined (epsilon at minimum)\n    is_baselined = agent.epsilon <= agent.epsilon_min\n    \n    # Check for new best - BY POINTS, not P&L\n    is_new_best = False\n    if total_reward > best_points and not stats.get('liquidated'):\n        best_points = total_reward\n        best_stats = stats.copy()\n        episodes_since_best = 0\n        plateau_counter = 0  # Reset plateau counter on new best\n        is_new_best = True\n        \n        if episode >= MIN_EXPLORATION_EPISODES:\n            agent.decay_epsilon()\n        \n        status = f\"BEST! ε→{agent.epsilon:.2f}\"\n    elif stats.get('liquidated'):\n        episodes_since_best += 1\n        if is_baselined:\n            plateau_counter += 1\n        status = \"LIQUIDATED\"\n    elif total_reward > 0:\n        episodes_since_best += 1\n        if is_baselined:\n            plateau_counter += 1\n        status = f\"+pts x{boost_factor}\"\n    else:\n        episodes_since_best += 1\n        if is_baselined:\n            plateau_counter += 1\n        status = f\"-pts x{boost_factor}\"\n    \n    # Print every episode\n    max_dd = stats.get('max_drawdown', 0)\n    ep_sharpe = stats.get('episode_sharpe', 0)\n    win_rate = stats.get('win_rate', 0)\n    \n    print(f\"{episode+1:5d} | {total_reward:+7.1f} | {stats['total_return_pct']:+7.1f}% | {max_dd:5.1f}% | {ep_sharpe:+5.2f} | \"\n          f\"{stats['num_trades']:6d} | {win_rate:4.1f}% | {agent.epsilon:.3f} | {status}\")\n    \n    # Progress summary every 50 episodes\n    if (episode + 1) % 50 == 0:\n        print(\"-\" * 100)\n        if not is_baselined:\n            phase = \"EXPLORATION\" if episode < MIN_EXPLORATION_EPISODES else f\"LEARNING (ε={agent.epsilon:.2f})\"\n        else:\n            phase = f\"BASELINED - plateau {plateau_counter}/{PLATEAU_THRESHOLD}\"\n        print(f\"Episode {episode+1} [{phase}] | Best points: {best_points:.1f} | \"\n              f\"Eps since best: {episodes_since_best}\")\n        if best_stats:\n            print(f\"Best episode: {best_stats.get('total_points', 0):.1f} pts, \"\n                  f\"{best_stats['total_return_pct']:.1f}% return, \"\n                  f\"{best_stats['num_trades']} trades, \"\n                  f\"{best_stats.get('win_rate', 0):.1f}% WR, \"\n                  f\"Sharpe: {best_stats.get('episode_sharpe', 0):.2f}\")\n        print(f\"Total experiences in memory: {len(agent.memory):,}\")\n        print(\"-\" * 100)\n    \n    episode += 1\n    \n    # Check for plateau - ONLY when baselined (epsilon at floor)\n    if is_baselined and plateau_counter >= PLATEAU_THRESHOLD:\n        print(\"\\n\" + \"=\"*100)\n        print(f\"PLATEAU REACHED: Baselined and no improvement for {plateau_counter} episodes. Stopping.\")\n        print(f\"Best points: {best_points:.1f} | Best return: {best_stats['total_return_pct']:.1f}%\")\n        print(\"=\"*100)\n        break\n    \n    # Safety max (very high)\n    if episode >= 10000:\n        print(\"\\nMax episodes reached (10000). Stopping.\")\n        break\n\nprint(f\"\\nTraining complete after {episode} episodes!\")\nprint(f\"Best total points: {best_points:.1f}\")\nprint(f\"Best P&L: {best_stats['total_return_pct']:.1f}%\")\nprint(f\"Total experiences collected: {len(agent.memory):,}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================\n# OUTLIER ANALYSIS - What patterns lead to big wins/losses?\n# ============================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"OUTLIER ANALYSIS\")\nprint(\"=\"*70)\n\nif all_trades:\n    trades_df = pd.DataFrame(all_trades)\n    \n    print(f\"\\nTotal trades analyzed: {len(trades_df):,}\")\n    \n    # Big wins (>5% account growth)\n    big_wins = trades_df[trades_df['account_pct_change'] > 5]\n    print(f\"\\n{'='*50}\")\n    print(f\"BIG WINS (>5% account growth): {len(big_wins)}\")\n    print(f\"{'='*50}\")\n    \n    if len(big_wins) > 0:\n        print(\"\\nAverage entry conditions for big wins:\")\n        for col in ['wt1', 'rsi', 'macd_hist', 'bb_pct', 'trend_score']:\n            vals = [t['entry_indicators'].get(col, 0) for t in big_wins.to_dict('records')]\n            print(f\"  {col}: {np.mean(vals):.2f}\")\n        \n        print(f\"\\nBig wins by position type:\")\n        print(f\"  Long:  {(big_wins['position'] > 0).sum()}\")\n        print(f\"  Short: {(big_wins['position'] < 0).sum()}\")\n        \n        print(f\"\\nAvg bars held for big wins: {big_wins['bars_held'].mean():.0f}\")\n        \n        print(\"\\nTop 3 biggest wins:\")\n        for i, trade in big_wins.nlargest(3, 'account_pct_change').iterrows():\n            entry = trade['entry_indicators']\n            print(f\"  +{trade['account_pct_change']:.1f}% | \"\n                  f\"{'LONG' if trade['position'] > 0 else 'SHORT'} | \"\n                  f\"Held {trade['bars_held']} bars | \"\n                  f\"RSI={entry.get('rsi', 0):.0f}, WT={entry.get('wt1', 0):.0f}\")\n    \n    # Big losses (>3% account loss)\n    big_losses = trades_df[trades_df['account_pct_change'] < -3]\n    print(f\"\\n{'='*50}\")\n    print(f\"BIG LOSSES (>3% account loss): {len(big_losses)}\")\n    print(f\"{'='*50}\")\n    \n    if len(big_losses) > 0:\n        print(\"\\nAverage entry conditions for big losses:\")\n        for col in ['wt1', 'rsi', 'macd_hist', 'bb_pct', 'trend_score']:\n            vals = [t['entry_indicators'].get(col, 0) for t in big_losses.to_dict('records')]\n            print(f\"  {col}: {np.mean(vals):.2f}\")\n        \n        print(f\"\\nBig losses by position type:\")\n        print(f\"  Long:  {(big_losses['position'] > 0).sum()}\")\n        print(f\"  Short: {(big_losses['position'] < 0).sum()}\")\n        \n        print(f\"\\nBig losses by exit reason:\")\n        print(big_losses['exit_reason'].value_counts().to_string())\n        \n        print(\"\\nTop 3 biggest losses:\")\n        for i, trade in big_losses.nsmallest(3, 'account_pct_change').iterrows():\n            entry = trade['entry_indicators']\n            print(f\"  {trade['account_pct_change']:.1f}% | \"\n                  f\"{'LONG' if trade['position'] > 0 else 'SHORT'} | \"\n                  f\"Exit: {trade['exit_reason']} | \"\n                  f\"RSI={entry.get('rsi', 0):.0f}, WT={entry.get('wt1', 0):.0f}\")\n    \n    # Pattern comparison\n    print(f\"\\n{'='*50}\")\n    print(\"PATTERN COMPARISON: Wins vs Losses\")\n    print(f\"{'='*50}\")\n    \n    wins = trades_df[trades_df['pnl_dollars'] > 0]\n    losses = trades_df[trades_df['pnl_dollars'] < 0]\n    \n    if len(wins) > 0 and len(losses) > 0:\n        print(f\"\\n{'Indicator':<15} | {'Wins Avg':>10} | {'Losses Avg':>10} | {'Diff':>10}\")\n        print(\"-\" * 50)\n        for col in ['wt1', 'rsi', 'macd_hist', 'bb_pct', 'trend_score', 'atr_pct']:\n            win_vals = [t['entry_indicators'].get(col, 0) for t in wins.to_dict('records')]\n            loss_vals = [t['entry_indicators'].get(col, 0) for t in losses.to_dict('records')]\n            win_avg = np.mean(win_vals)\n            loss_avg = np.mean(loss_vals)\n            diff = win_avg - loss_avg\n            print(f\"{col:<15} | {win_avg:>10.2f} | {loss_avg:>10.2f} | {diff:>+10.2f}\")\n    \n    # Exit reason analysis\n    print(f\"\\n{'='*50}\")\n    print(\"EXIT REASON ANALYSIS\")\n    print(f\"{'='*50}\")\n    print(trades_df.groupby('exit_reason')['pnl_dollars'].agg(['count', 'mean', 'sum']).round(2))\n    \nelse:\n    print(\"No trades to analyze.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# TEST THE TRAINED AGENT\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING TRAINED AGENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test on last 20% of data (unseen)\n",
    "test_start = int(len(df) * 0.8)\n",
    "test_df = df.iloc[test_start:].copy()\n",
    "print(f\"\\nTest period: {test_df.index.min().date()} to {test_df.index.max().date()}\")\n",
    "print(f\"Test bars: {len(test_df):,}\")\n",
    "\n",
    "# Create test environment\n",
    "test_env = TradingEnvironment(test_df, initial_capital=1000)\n",
    "\n",
    "# Run test episode (no exploration)\n",
    "agent.epsilon = 0  # Pure exploitation\n",
    "state = test_env.reset(start_idx=0)\n",
    "\n",
    "while not test_env.done:\n",
    "    action_idx = agent.choose_action(state, training=False)\n",
    "    action = agent.action_to_dict(action_idx, state['max_units'])\n",
    "    state, reward, done, info = test_env.step(action)\n",
    "\n",
    "# Get results\n",
    "test_stats = test_env.get_episode_stats()\n",
    "\n",
    "print(f\"\\n{'Metric':<20} {'Value':>15}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"{'Starting Capital':<20} ${test_stats['starting_capital']:>14,.2f}\")\n",
    "print(f\"{'Ending Capital':<20} ${test_stats['ending_capital']:>14,.2f}\")\n",
    "print(f\"{'Total Return':<20} {test_stats['total_return_pct']:>14.1f}%\")\n",
    "print(f\"{'Trades':<20} {test_stats['num_trades']:>15,}\")\n",
    "print(f\"{'Win Rate':<20} {test_stats.get('win_rate', 0):>14.1f}%\")\n",
    "print(f\"{'Avg Win':<20} ${test_stats.get('avg_win', 0):>14,.2f}\")\n",
    "print(f\"{'Avg Loss':<20} ${test_stats.get('avg_loss', 0):>14,.2f}\")\n",
    "print(f\"{'Best Trade':<20} ${test_stats.get('best_trade', 0):>14,.2f}\")\n",
    "print(f\"{'Worst Trade':<20} ${test_stats.get('worst_trade', 0):>14,.2f}\")\n",
    "print(f\"{'Max Drawdown':<20} {test_stats.get('max_drawdown', 0):>14.1f}%\")\n",
    "print(f\"{'Liquidated':<20} {'YES' if test_stats['liquidated'] else 'NO':>15}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# TEST EQUITY CURVE\n",
    "# ============================================================\n",
    "\n",
    "if len(test_env.equity_curve) > 1:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    equity = test_env.equity_curve\n",
    "    plt.plot(equity, linewidth=2)\n",
    "    plt.axhline(y=1000, color='gray', linestyle='--', label='Starting capital')\n",
    "    plt.axhline(y=300, color='red', linestyle='--', label='Liquidation level')\n",
    "    \n",
    "    plt.xlabel('Bar')\n",
    "    plt.ylabel('Equity ($)')\n",
    "    plt.title(f'Test Period Equity Curve\\nFinal: ${equity[-1]:,.2f} ({test_stats[\"total_return_pct\"]:+.1f}%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# TRADE ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "if test_env.trades:\n",
    "    trades_df = pd.DataFrame(test_env.trades)\n",
    "    \n",
    "    print(\"\\nTrade Distribution:\")\n",
    "    print(f\"  Long trades:  {(trades_df['position'] > 0).sum()}\")\n",
    "    print(f\"  Short trades: {(trades_df['position'] < 0).sum()}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(trades_df['pnl_dollars'], bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.axvline(x=0, color='red', linestyle='--')\n",
    "    plt.xlabel('P&L ($)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Trade P&L Distribution')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    cumulative_pnl = trades_df['pnl_dollars'].cumsum()\n",
    "    plt.plot(cumulative_pnl, linewidth=2)\n",
    "    plt.axhline(y=0, color='gray', linestyle='--')\n",
    "    plt.xlabel('Trade #')\n",
    "    plt.ylabel('Cumulative P&L ($)')\n",
    "    plt.title('Cumulative P&L by Trade')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================\n# SAVE MODEL\n# ============================================================\n\nimport os\nos.makedirs('models/rl_v1', exist_ok=True)\n\n# Save agent components\njoblib.dump(agent.model, 'models/rl_v1/q_network.pkl')\njoblib.dump(agent.scaler, 'models/rl_v1/scaler.pkl')\n\n# Save config\nconfig = {\n    'version': 'rl_v1',\n    'type': 'reinforcement_learning',\n    'goal': 'Maximum profit through learning',\n    'training_episodes': episode,\n    'action_space': agent.action_size,\n    'state_size': agent.state_size,\n    'trading_params': {\n        'initial_capital': 1000,\n        'margin_per_unit': 36.57,\n        'unit_size_oz': 0.15,\n        'spread_pct': 0.0004,\n        'liquidation_pct': 0.30\n    },\n    'test_performance': {\n        'period': f\"{test_df.index.min().date()} to {test_df.index.max().date()}\",\n        'starting_capital': test_stats['starting_capital'],\n        'ending_capital': test_stats['ending_capital'],\n        'total_return_pct': test_stats['total_return_pct'],\n        'trades': test_stats['num_trades'],\n        'win_rate': test_stats.get('win_rate', 0),\n        'max_drawdown': test_stats.get('max_drawdown', 0),\n        'liquidated': test_stats['liquidated']\n    },\n    'best_training_episode': best_stats\n}\n\nwith open('models/rl_v1/config.json', 'w') as f:\n    json.dump(config, f, indent=2, default=str)\n\nprint(\"Model saved to models/rl_v1/\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# DOWNLOAD MODEL\n",
    "# ============================================================\n",
    "\n",
    "!cd models && zip -r rl_v1.zip rl_v1/\n",
    "\n",
    "from google.colab import files\n",
    "files.download('models/rl_v1.zip')\n",
    "\n",
    "print(\"\\nDownload started!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================\n# SUMMARY\n# ============================================================\n\n# Calculate liquidation rate from episode stats\nstats_df = pd.DataFrame(episode_stats)\nliquidation_rate = stats_df['liquidated'].mean() * 100 if len(stats_df) > 0 else 0\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"RL TRADING AGENT v1.0 - COMPLETE\")\nprint(\"=\"*60)\nprint(f\"\"\"\nThis agent learned to trade Gold by doing.\n\nTraining:\n- {episode} episodes of trial and error\n- Started with $1,000 each episode\n- Could go long, short, pyramid, use stops\n- Learned from blowups and wins\n\nWhat it learned:\n- Best episode return: {best_stats['total_return_pct']:.1f}% (training)\n- Liquidation rate: {liquidation_rate:.1f}%\n\nTest results ({test_df.index.min().date()} to {test_df.index.max().date()}):\n- Return: {test_stats['total_return_pct']:.1f}%\n- Trades: {test_stats['num_trades']}\n- Win Rate: {test_stats.get('win_rate', 0):.1f}%\n- Max Drawdown: {test_stats.get('max_drawdown', 0):.1f}%\n\nModel saved and ready to download.\n\"\"\")",
   "execution_count": null,
   "outputs": []
  }
 ]
}