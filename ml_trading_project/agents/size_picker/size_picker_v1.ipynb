{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Size Picker Agent V2\n\n**Task**: Pick optimal position size based on candle patterns\n\n**Input**: OHLC candles (normalized to log returns + features) + Equity\n\n**Output**: Two heads predicting for each of 30 sizes:\n- P(liquidation) - probability the trade will liquidate at this size\n- E[return] - expected return at this size\n\n**Decision Rule**: Pick largest size where P(liq) < threshold, with best expected return\n\n**Goal**: Maximize return without getting liquidated (30% equity = liquidation)\n\n## V2 Changes (from expert review)\n\n1. **Two-head model** instead of 30-class classification\n2. **Asymmetric loss** - penalize predicting \"safe\" when actually liquidates\n3. **Better normalization** - log returns + candle features (range, body, wicks)\n4. **Training stability** - AdamW, gradient clipping, OneCycleLR\n5. **Risk-aware metrics** - liquidation rate, regret (not just accuracy)\n\n## Configuration\n\nAll tunable parameters in the config cell:\n- `EPOCHS`, `BATCH_SIZE`, `LEARNING_RATE`, `HIDDEN_SIZE`\n- `GRAD_CLIP`, `WEIGHT_DECAY`, `LIQ_THRESHOLD`\n- `DATASET_NAME` - which V2 dataset to train on\n- `LOAD_FROM` - checkpoint filename to resume\n\n## Training Data (V2)\n\nRequires datasets with `all_results` field (30 outcomes per trade):\n- `balanced_v2_100k` - 33k each of small/mid/big optimal\n- Generate using `generate_all_datasets_v2()` in generate_datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport os\nimport pickle\nimport random\n\ntry:\n    import google.colab\n    IN_COLAB = True\nexcept:\n    IN_COLAB = False\n\nif IN_COLAB:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    DATA_DIR = '/content/drive/MyDrive/size_picker_data'\n    print(f\"Google Drive mounted. Data dir: {DATA_DIR}\")\nelse:\n    DATA_DIR = 'data'\n    print(f\"Running locally. Data dir: {DATA_DIR}\")\n\nos.makedirs('models', exist_ok=True)\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport matplotlib.pyplot as plt\n\n# GPU Support\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Device: {device}\")\nif device.type == 'cuda':\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "code",
   "id": "xz4v5rb2il",
   "source": "# =============================================================================\n# CONFIGURATION - V2 Parameters\n# =============================================================================\n\n# Training settings\nEPOCHS = 10000\nBATCH_SIZE = 32\nLEARNING_RATE = 0.001\nGRAD_CLIP = 1.0\nWEIGHT_DECAY = 0.01\n\n# Model settings\nHIDDEN_SIZE = 128\nNUM_LSTM_LAYERS = 2\nDROPOUT = 0.2\nINPUT_FEATURES = 5  # log_return, range, body, upper_wick, lower_wick\n\n# Decision settings\nLIQ_THRESHOLD = 0.1  # Max acceptable P(liquidation) for size selection\n\n# Loss weights\nLOSS_LIQ_WEIGHT = 1.0\nLOSS_RETURN_WEIGHT = 0.1\nLOSS_ASYMMETRIC_WEIGHT = 5.0\n\n# Data settings\nDATASET_NAME = 'balanced_v2_100k'  # Requires V2 dataset with all_results\nTRAIN_SPLIT = 0.8\n\n# Checkpoint settings\nLOAD_FROM = None                # Set to checkpoint filename to resume, or None for fresh\nSAVE_EVERY = 100                # Save checkpoint every N epochs\n\nprint(\"V2 Configuration loaded:\")\nprint(f\"  EPOCHS: {EPOCHS}\")\nprint(f\"  BATCH_SIZE: {BATCH_SIZE}\")\nprint(f\"  LEARNING_RATE: {LEARNING_RATE}\")\nprint(f\"  GRAD_CLIP: {GRAD_CLIP}\")\nprint(f\"  WEIGHT_DECAY: {WEIGHT_DECAY}\")\nprint(f\"  HIDDEN_SIZE: {HIDDEN_SIZE}\")\nprint(f\"  NUM_LSTM_LAYERS: {NUM_LSTM_LAYERS}\")\nprint(f\"  DROPOUT: {DROPOUT}\")\nprint(f\"  LIQ_THRESHOLD: {LIQ_THRESHOLD}\")\nprint(f\"  DATASET_NAME: {DATASET_NAME}\")\nprint(f\"  LOAD_FROM: {LOAD_FROM or 'Training from scratch'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Load datasets from Google Drive\ndef load_dataset(name):\n    filepath = f'{DATA_DIR}/{name}.pkl'\n    with open(filepath, 'rb') as f:\n        trades = pickle.load(f)\n    print(f\"Loaded {len(trades)} trades from {name}\")\n    return trades\n\ndef list_datasets():\n    files = [f for f in os.listdir(DATA_DIR) if f.endswith('.pkl')]\n    print(\"Available datasets:\")\n    for f in files:\n        filepath = f'{DATA_DIR}/{f}'\n        with open(filepath, 'rb') as file:\n            trades = pickle.load(file)\n        print(f\"  {f}: {len(trades)} trades\")\n    return files\n\n# List available datasets\nlist_datasets()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Valid sizes and constants\nSIZES = [round(0.15 * i, 2) for i in range(1, 31)]  # 0.15 to 4.50\nNUM_SIZES = len(SIZES)\nLOOKBACK = 24\nEQUITY = 1000\n\nprint(f\"Valid sizes: {SIZES[0]} to {SIZES[-1]} ({NUM_SIZES} sizes)\")\nprint(f\"Lookback: {LOOKBACK} candles\")\n\n# Categories\nSMALL_SIZES = [s for s in SIZES if s <= 1.5]      # 0.15 to 1.50\nMID_SIZES = [s for s in SIZES if 1.5 < s <= 3.0]  # 1.65 to 3.00\nBIG_SIZES = [s for s in SIZES if s > 3.0]         # 3.15 to 4.50\n\nprint(f\"Small: {len(SMALL_SIZES)} sizes ({SMALL_SIZES[0]}-{SMALL_SIZES[-1]})\")\nprint(f\"Mid: {len(MID_SIZES)} sizes ({MID_SIZES[0]}-{MID_SIZES[-1]})\")\nprint(f\"Big: {len(BIG_SIZES)} sizes ({BIG_SIZES[0]}-{BIG_SIZES[-1]})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Load training data\nall_trades = load_dataset(DATASET_NAME)\n\n# Verify distribution\ncategories = [t['category'] for t in all_trades]\nprint(f\"\\nCategory distribution:\")\nfor cat in ['small', 'mid', 'big']:\n    count = categories.count(cat)\n    print(f\"  {cat}: {count} ({count/len(all_trades)*100:.1f}%)\")\n\n# Show sample trade structure\nprint(f\"\\nSample trade keys: {list(all_trades[0].keys())}\")\nprint(f\"Candle shape: {all_trades[0]['candles'].shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Helper functions - V2 normalization\n\ndef normalize_candles_v2(candles):\n    \"\"\"\n    V2 normalization: Extract meaningful features from OHLC candles.\n    \n    Instead of raw price percent changes, extract:\n    - Log returns (close-to-close) - captures momentum\n    - Range (high-low)/close - captures volatility\n    - Body |close-open|/close - captures conviction\n    - Upper wick - captures rejection from highs\n    - Lower wick - captures rejection from lows\n    \n    Returns: [seq_len-1, 5] array (one fewer row due to returns)\n    \"\"\"\n    # Ensure numpy array\n    candles = np.array(candles)\n    \n    # Log returns (close-to-close)\n    closes = candles[:, 3]\n    log_returns = np.log(closes[1:] / closes[:-1])\n    \n    # Per-candle features (skip first since we use returns starting from second)\n    opens = candles[1:, 0]\n    highs = candles[1:, 1]\n    lows = candles[1:, 2]\n    closes_shifted = candles[1:, 3]\n    \n    # Range: (high - low) / close - volatility measure\n    ranges = (highs - lows) / closes_shifted\n    \n    # Body: |close - open| / close - conviction measure\n    bodies = np.abs(closes_shifted - opens) / closes_shifted\n    \n    # Upper wick: (high - max(open, close)) / close\n    upper_wicks = (highs - np.maximum(opens, closes_shifted)) / closes_shifted\n    \n    # Lower wick: (min(open, close) - low) / close\n    lower_wicks = (np.minimum(opens, closes_shifted) - lows) / closes_shifted\n    \n    # Stack features: [log_return, range, body, upper_wick, lower_wick]\n    features = np.column_stack([log_returns, ranges, bodies, upper_wicks, lower_wicks])\n    \n    return features\n\n# Legacy function for backwards compatibility\ndef normalize_candles(candles):\n    \"\"\"Legacy normalization - relative to first close price\"\"\"\n    base = candles[0, 3]\n    return (candles - base) / base * 100\n\n# Test V2 normalization\nsample = all_trades[0]\ncandles_v2 = normalize_candles_v2(sample['candles'])\nprint(f\"V2 normalized shape: {candles_v2.shape}\")\nprint(f\"Feature names: [log_return, range, body, upper_wick, lower_wick]\")\nprint(f\"Sample features (first 3 bars):\")\nfor i in range(min(3, candles_v2.shape[0])):\n    print(f\"  Bar {i}: ret={candles_v2[i,0]:.4f}, rng={candles_v2[i,1]:.4f}, body={candles_v2[i,2]:.4f}, uwk={candles_v2[i,3]:.4f}, lwk={candles_v2[i,4]:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# Neural network - V2 Two-Head Architecture\n\nclass SizePickerV2(nn.Module):\n    \"\"\"\n    Two-head model for position sizing.\n    \n    Instead of 30-class classification, predicts:\n    - P(liquidation) for each of 30 sizes\n    - E[return] for each of 30 sizes\n    \n    Decision: Pick largest size where P(liq) < threshold with best expected return.\n    \"\"\"\n    \n    def __init__(self, input_size=5, hidden_size=128, num_layers=2, dropout=0.2, num_sizes=30):\n        super().__init__()\n        self.num_sizes = num_sizes\n        self.hidden_size = hidden_size\n        \n        # Encoder: LSTM with dropout between layers\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        \n        # Layer normalization for stability\n        self.layer_norm = nn.LayerNorm(hidden_size)\n        \n        # Shared representation (LSTM output + equity)\n        self.shared = nn.Sequential(\n            nn.Linear(hidden_size + 1, 64),  # +1 for equity\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        \n        # Head 1: P(liquidation) for each size - 30 sigmoid outputs\n        self.liq_head = nn.Sequential(\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, num_sizes),\n            nn.Sigmoid()  # Output probabilities [0, 1]\n        )\n        \n        # Head 2: E[return] for each size - 30 outputs (unbounded)\n        self.return_head = nn.Sequential(\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, num_sizes)\n        )\n    \n    def forward(self, candles, equity):\n        \"\"\"\n        Forward pass.\n        \n        Args:\n            candles: [batch, seq_len, 5] normalized candle features\n            equity: [batch, 1] normalized equity\n            \n        Returns:\n            p_liq: [batch, 30] P(liquidation) for each size\n            e_return: [batch, 30] E[return] for each size\n        \"\"\"\n        # LSTM encoding\n        lstm_out, _ = self.lstm(candles)\n        last_hidden = lstm_out[:, -1, :]  # Take last timestep\n        \n        # Normalize for stability\n        last_hidden = self.layer_norm(last_hidden)\n        \n        # Combine with equity\n        combined = torch.cat([last_hidden, equity], dim=1)\n        \n        # Shared representation\n        shared = self.shared(combined)\n        \n        # Two heads\n        p_liq = self.liq_head(shared)       # [batch, 30]\n        e_return = self.return_head(shared)  # [batch, 30]\n        \n        return p_liq, e_return\n    \n    def pick_size(self, candles, equity, liq_threshold=0.1):\n        \"\"\"\n        Choose optimal size: largest safe size with best expected return.\n        \n        Args:\n            candles: [batch, seq_len, 5]\n            equity: [batch, 1]\n            liq_threshold: max acceptable P(liquidation)\n            \n        Returns:\n            size_idx: [batch] index of chosen size\n        \"\"\"\n        with torch.no_grad():\n            p_liq, e_return = self.forward(candles, equity)\n            \n            # Mask sizes with P(liq) > threshold\n            safe_mask = p_liq < liq_threshold\n            \n            # Set unsafe sizes to -inf return so they won't be picked\n            masked_returns = torch.where(\n                safe_mask,\n                e_return,\n                torch.tensor(float('-inf'), device=e_return.device)\n            )\n            \n            # Pick size with best return among safe options\n            idx = masked_returns.argmax(dim=1)\n            \n        return idx\n\n\n# Create model and move to GPU\nmodel = SizePickerV2(\n    input_size=INPUT_FEATURES,\n    hidden_size=HIDDEN_SIZE,\n    num_layers=NUM_LSTM_LAYERS,\n    dropout=DROPOUT,\n    num_sizes=NUM_SIZES\n).to(device)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Model: SizePickerV2\")\nprint(f\"  LSTM({INPUT_FEATURES} -> {HIDDEN_SIZE}, {NUM_LSTM_LAYERS} layers)\")\nprint(f\"  Outputs: P(liq)[{NUM_SIZES}] + E[return][{NUM_SIZES}]\")\nprint(f\"  Total parameters: {total_params:,}\")\nprint(f\"  Trainable parameters: {trainable_params:,}\")\nprint(f\"  Device: {device}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# Training setup - V2 Dataset with all_results\n\n# Find max sequence length in dataset (after V2 normalization, length is n-1)\nmax_len = max(t['candles'].shape[0] - 1 for t in all_trades)  # -1 because V2 uses returns\nprint(f\"Max candle sequence length (after V2 norm): {max_len}\")\n\nclass TradeDatasetV2(Dataset):\n    \"\"\"\n    V2 Dataset that returns all_results for multi-task learning.\n    \n    Returns:\n        candles: [seq_len, 5] V2 normalized features\n        equity: [1] normalized equity\n        target: int - optimal size index\n        liq_targets: [30] binary - did each size liquidate?\n        return_targets: [30] float - return % for each size\n    \"\"\"\n    \n    def __init__(self, trades, max_len):\n        self.trades = trades\n        self.max_len = max_len\n        \n        # Verify trades have all_results\n        if 'all_results' not in trades[0]:\n            raise ValueError(\"Dataset missing 'all_results' field. Use V2 datasets from generate_all_datasets_v2()\")\n    \n    def __len__(self):\n        return len(self.trades)\n    \n    def __getitem__(self, idx):\n        trade = self.trades[idx]\n        \n        # V2 normalization\n        candles_norm = normalize_candles_v2(trade['candles'])\n        \n        # Pad to max length\n        seq_len = candles_norm.shape[0]\n        if seq_len < self.max_len:\n            padding = np.zeros((self.max_len - seq_len, INPUT_FEATURES))\n            candles_norm = np.vstack([candles_norm, padding])\n        \n        candles_t = torch.FloatTensor(candles_norm)\n        equity_t = torch.FloatTensor([EQUITY / 1000])  # Normalized equity\n        target = SIZES.index(trade['optimal_size'])\n        \n        # Build targets from all_results\n        all_results = trade['all_results']\n        liq_targets = torch.zeros(NUM_SIZES)\n        return_targets = torch.zeros(NUM_SIZES)\n        \n        for i, result in enumerate(all_results):\n            liq_targets[i] = float(result['liquidated'])\n            return_targets[i] = result['return_pct']\n        \n        return candles_t, equity_t, target, liq_targets, return_targets\n\n\n# Check if dataset has all_results\nhas_all_results = 'all_results' in all_trades[0]\nprint(f\"Dataset has all_results: {has_all_results}\")\n\nif not has_all_results:\n    print(\"\\nWARNING: Current dataset missing 'all_results' field!\")\n    print(\"You need to regenerate using generate_all_datasets_v2()\")\n    print(\"The training loop will fail without V2 datasets.\")\nelse:\n    # Verify all_results structure\n    sample_results = all_trades[0]['all_results']\n    print(f\"  all_results length: {len(sample_results)}\")\n    print(f\"  Sample result: {sample_results[0]}\")\n\n# Split data\nrandom.shuffle(all_trades)\nsplit_idx = int(len(all_trades) * TRAIN_SPLIT)\ntrain_trades = all_trades[:split_idx]\nval_trades = all_trades[split_idx:]\n\nif has_all_results:\n    train_dataset = TradeDatasetV2(train_trades, max_len)\n    val_dataset = TradeDatasetV2(val_trades, max_len)\n    \n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    print(f\"\\nTrain: {len(train_trades)} trades, {len(train_loader)} batches\")\n    print(f\"Val: {len(val_trades)} trades, {len(val_loader)} batches\")\nelse:\n    print(\"\\nDataLoaders NOT created - need V2 datasets\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# Training loop - V2 with asymmetric loss\n\n# Loss function\ndef compute_loss(p_liq, e_return, liq_targets, return_targets):\n    \"\"\"\n    Combined loss with asymmetric penalties.\n    \n    Args:\n        p_liq: [batch, 30] predicted P(liquidation)\n        e_return: [batch, 30] predicted E[return]\n        liq_targets: [batch, 30] actual liquidation (0 or 1)\n        return_targets: [batch, 30] actual returns\n        \n    Returns:\n        total_loss, loss_parts dict\n    \"\"\"\n    # Liquidation prediction loss (BCE)\n    liq_loss = F.binary_cross_entropy(p_liq, liq_targets)\n    \n    # Return prediction loss (Huber for robustness to outliers)\n    return_loss = F.huber_loss(e_return, return_targets)\n    \n    # Asymmetric penalty: heavily penalize predicting \"safe\" when actually liquidates\n    # This is the critical safety component\n    false_safe = (p_liq < 0.5) & (liq_targets == 1)\n    asymmetric_penalty = (false_safe.float() * LOSS_ASYMMETRIC_WEIGHT).mean()\n    \n    # Weighted combination\n    total_loss = (LOSS_LIQ_WEIGHT * liq_loss + \n                  LOSS_RETURN_WEIGHT * return_loss + \n                  asymmetric_penalty)\n    \n    return total_loss, {\n        'liq': liq_loss.item(),\n        'ret': return_loss.item(),\n        'asym': asymmetric_penalty.item(),\n        'total': total_loss.item()\n    }\n\n# Metrics\ndef compute_metrics(model, loader, liq_threshold=LIQ_THRESHOLD):\n    \"\"\"\n    Compute risk-aware metrics.\n    \n    Returns:\n        dict with liquidation_rate, avg_return, optimal_return, regret\n    \"\"\"\n    model.eval()\n    \n    total_trades = 0\n    liquidations = 0\n    total_return = 0\n    optimal_return = 0\n    correct_category = 0\n    \n    with torch.no_grad():\n        for candles, equity, targets, liq_targets, return_targets in loader:\n            # Move data to GPU\n            candles = candles.to(device)\n            equity = equity.to(device)\n            liq_targets = liq_targets.to(device)\n            return_targets = return_targets.to(device)\n            \n            p_liq, e_return = model(candles, equity)\n            chosen_idx = model.pick_size(candles, equity, liq_threshold)\n            \n            batch_size = candles.shape[0]\n            for i in range(batch_size):\n                chosen = chosen_idx[i].item()\n                optimal = targets[i].item()\n                \n                # Did chosen size liquidate?\n                if liq_targets[i, chosen].item() > 0.5:\n                    liquidations += 1\n                    actual_ret = -70  # 70% loss on liquidation\n                else:\n                    actual_ret = return_targets[i, chosen].item()\n                \n                # Optimal return (what we could have gotten)\n                opt_ret = return_targets[i, optimal].item()\n                \n                total_return += actual_ret\n                optimal_return += opt_ret\n                total_trades += 1\n                \n                # Category accuracy (for comparison with V1)\n                chosen_size = SIZES[chosen]\n                optimal_size = SIZES[optimal]\n                if get_category(chosen_size) == get_category(optimal_size):\n                    correct_category += 1\n    \n    return {\n        'liquidation_rate': liquidations / total_trades * 100,\n        'avg_return': total_return / total_trades,\n        'optimal_return': optimal_return / total_trades,\n        'regret': (optimal_return - total_return) / total_trades,\n        'category_acc': correct_category / total_trades * 100\n    }\n\ndef get_category(size):\n    if size <= 1.5: return 'small'\n    elif size <= 3.0: return 'mid'\n    else: return 'big'\n\n# Optimizer with weight decay\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\n# OneCycleLR scheduler\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=LEARNING_RATE * 10,  # Peak at 10x base LR\n    epochs=EPOCHS,\n    steps_per_epoch=len(train_loader),\n    pct_start=0.1,  # Warmup for 10% of training\n    anneal_strategy='cos'\n)\n\n# Checkpoint directory\nCHECKPOINT_DIR = f'{DATA_DIR}/checkpoints_v2'\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\n# Load checkpoint if specified\nstart_epoch = 0\nbest_regret = float('inf')\nhistory = {'train_loss': [], 'val_liq_rate': [], 'val_return': [], 'val_regret': [], 'lr': []}\n\nif LOAD_FROM:\n    checkpoint_path = f'{CHECKPOINT_DIR}/{LOAD_FROM}' if not LOAD_FROM.startswith('/') else LOAD_FROM\n    if os.path.exists(checkpoint_path):\n        print(f\"Loading checkpoint: {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        if 'optimizer_state_dict' in checkpoint:\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        if 'scheduler_state_dict' in checkpoint:\n            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        if 'epoch' in checkpoint:\n            start_epoch = checkpoint['epoch']\n        if 'history' in checkpoint:\n            history = checkpoint['history']\n        if 'best_regret' in checkpoint:\n            best_regret = checkpoint['best_regret']\n        print(f\"  Resumed from epoch {start_epoch}\")\n    else:\n        print(f\"Checkpoint not found: {checkpoint_path}, starting fresh\")\n\ndef train_epoch(model, loader, optimizer, scheduler):\n    model.train()\n    total_loss = 0\n    loss_parts_sum = {'liq': 0, 'ret': 0, 'asym': 0}\n    \n    for candles, equity, targets, liq_targets, return_targets in loader:\n        # Move data to GPU\n        candles = candles.to(device)\n        equity = equity.to(device)\n        liq_targets = liq_targets.to(device)\n        return_targets = return_targets.to(device)\n        \n        optimizer.zero_grad()\n        \n        p_liq, e_return = model(candles, equity)\n        loss, loss_parts = compute_loss(p_liq, e_return, liq_targets, return_targets)\n        \n        loss.backward()\n        \n        # Gradient clipping for stability\n        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n        \n        optimizer.step()\n        scheduler.step()\n        \n        total_loss += loss.item()\n        for k in loss_parts_sum:\n            loss_parts_sum[k] += loss_parts[k]\n    \n    n = len(loader)\n    return total_loss / n, {k: v / n for k, v in loss_parts_sum.items()}\n\ndef save_checkpoint(model, optimizer, scheduler, epoch, history, metrics, best_regret, filename):\n    checkpoint = {\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'epoch': epoch,\n        'history': history,\n        'metrics': metrics,\n        'best_regret': best_regret,\n        'config': {\n            'hidden_size': HIDDEN_SIZE,\n            'num_layers': NUM_LSTM_LAYERS,\n            'dropout': DROPOUT,\n            'learning_rate': LEARNING_RATE,\n            'batch_size': BATCH_SIZE,\n            'liq_threshold': LIQ_THRESHOLD,\n            'dataset': DATASET_NAME\n        }\n    }\n    torch.save(checkpoint, f\"{CHECKPOINT_DIR}/{filename}\")\n\n# Train\nprint(f\"\\nV2 Training from epoch {start_epoch + 1} to {EPOCHS}\")\nprint(f\"Checkpoints: {CHECKPOINT_DIR}\")\nprint(f\"Best regret so far: {best_regret:.2f}%\")\nprint(f\"Device: {device}\\n\")\nprint(f\"{'Epoch':>6} | {'Loss':>8} | {'Liq%':>6} | {'Return':>8} | {'Regret':>8} | {'CatAcc':>6} | {'LR':>10}\")\nprint(\"-\" * 75)\n\nfor epoch in range(start_epoch, EPOCHS):\n    train_loss, loss_parts = train_epoch(model, train_loader, optimizer, scheduler)\n    metrics = compute_metrics(model, val_loader, LIQ_THRESHOLD)\n    \n    current_lr = scheduler.get_last_lr()[0]\n    \n    history['train_loss'].append(train_loss)\n    history['val_liq_rate'].append(metrics['liquidation_rate'])\n    history['val_return'].append(metrics['avg_return'])\n    history['val_regret'].append(metrics['regret'])\n    history['lr'].append(current_lr)\n    \n    # Check for new best (minimize regret)\n    is_best = metrics['regret'] < best_regret\n    if is_best:\n        best_regret = metrics['regret']\n        save_checkpoint(model, optimizer, scheduler, epoch + 1, history, metrics, best_regret, 'best.pt')\n    \n    # Save periodic checkpoint\n    if (epoch + 1) % SAVE_EVERY == 0:\n        save_checkpoint(model, optimizer, scheduler, epoch + 1, history, metrics, best_regret, f'epoch_{epoch+1}.pt')\n        marker = \"SAVED\"\n    elif is_best:\n        marker = \"*** BEST ***\"\n    else:\n        marker = \"\"\n    \n    print(f\"{epoch+1:6d} | {train_loss:8.4f} | {metrics['liquidation_rate']:5.1f}% | \"\n          f\"{metrics['avg_return']:7.2f}% | {metrics['regret']:7.2f}% | \"\n          f\"{metrics['category_acc']:5.1f}% | {current_lr:.2e} {marker}\")\n\nprint(f\"\\nTraining complete!\")\nprint(f\"Best regret: {best_regret:.2f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Plot V2 training curves\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Loss\nax = axes[0, 0]\nax.plot(history['train_loss'], label='Train Loss')\nax.set_xlabel('Epoch')\nax.set_ylabel('Loss')\nax.set_title('Training Loss')\nax.legend()\n\n# Liquidation Rate\nax = axes[0, 1]\nax.plot(history['val_liq_rate'], label='Liq Rate %', color='red')\nax.set_xlabel('Epoch')\nax.set_ylabel('Liquidation Rate %')\nax.set_title('Validation Liquidation Rate (lower is better)')\nax.axhline(y=10, color='gray', linestyle='--', alpha=0.5, label='10% target')\nax.legend()\n\n# Return and Regret\nax = axes[1, 0]\nax.plot(history['val_return'], label='Avg Return %', color='green')\nax.plot(history['val_regret'], label='Regret %', color='orange')\nax.set_xlabel('Epoch')\nax.set_ylabel('%')\nax.set_title('Return vs Regret (higher return, lower regret is better)')\nax.legend()\n\n# Learning Rate\nax = axes[1, 1]\nax.plot(history['lr'], label='Learning Rate', color='purple')\nax.set_xlabel('Epoch')\nax.set_ylabel('LR')\nax.set_title('Learning Rate Schedule (OneCycleLR)')\nax.set_yscale('log')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Final metrics\nmetrics = compute_metrics(model, val_loader, LIQ_THRESHOLD)\nprint(f\"\\nFinal Validation Metrics:\")\nprint(f\"  Liquidation Rate: {metrics['liquidation_rate']:.1f}%\")\nprint(f\"  Average Return: {metrics['avg_return']:.2f}%\")\nprint(f\"  Optimal Return: {metrics['optimal_return']:.2f}%\")\nprint(f\"  Regret: {metrics['regret']:.2f}%\")\nprint(f\"  Category Accuracy: {metrics['category_acc']:.1f}%\")"
  },
  {
   "cell_type": "code",
   "id": "5hkum8j2el5",
   "source": "# Hidden State Analysis - V2\n# What patterns is the LSTM learning?\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef extract_hidden_states_v2(model, trades, max_len, max_samples=2000):\n    \"\"\"Extract LSTM hidden states from V2 model\"\"\"\n    model.eval()\n    hidden_states = []\n    categories = []\n    \n    sample_trades = trades[:max_samples] if len(trades) > max_samples else trades\n    \n    with torch.no_grad():\n        for trade in sample_trades:\n            # V2 normalization\n            candles_norm = normalize_candles_v2(trade['candles'])\n            if candles_norm.shape[0] < max_len:\n                padding = np.zeros((max_len - candles_norm.shape[0], INPUT_FEATURES))\n                candles_norm = np.vstack([candles_norm, padding])\n            \n            candles_t = torch.FloatTensor(candles_norm).unsqueeze(0).to(device)\n            \n            # Extract hidden state (before heads)\n            lstm_out, _ = model.lstm(candles_t)\n            last_hidden = model.layer_norm(lstm_out[:, -1, :])\n            \n            hidden_states.append(last_hidden[0].cpu().numpy())\n            categories.append(trade['category'])\n    \n    return np.array(hidden_states), categories\n\n# Extract hidden states from validation set\nprint(\"Extracting V2 hidden states...\")\nhidden_states, categories = extract_hidden_states_v2(model, val_trades, max_len)\nprint(f\"Extracted {len(hidden_states)} hidden states, shape: {hidden_states.shape}\")\n\n# Map categories to numbers for visualization\ncat_to_num = {'small': 0, 'mid': 1, 'big': 2}\ncat_labels = [cat_to_num[c] for c in categories]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "mckj6o7qwyn",
   "source": "# PCA Visualization - do categories cluster separately?\npca = PCA(n_components=2)\nhidden_2d = pca.fit_transform(hidden_states)\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\ncolors = {'small': 'blue', 'mid': 'green', 'big': 'red'}\nfor cat in ['small', 'mid', 'big']:\n    mask = [c == cat for c in categories]\n    points = hidden_2d[mask]\n    ax.scatter(points[:, 0], points[:, 1], c=colors[cat], label=cat, alpha=0.5, s=20)\n\nax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')\nax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')\nax.set_title('Hidden State Clustering\\n(Do small/mid/big separate?)')\nax.legend()\nplt.tight_layout()\nplt.show()\n\n# Silhouette score - quantifies cluster quality\n# -1 = wrong clusters, 0 = overlapping, 1 = perfect separation\nsil_score = silhouette_score(hidden_states, cat_labels)\nprint(f\"\\nSilhouette Score: {sil_score:.3f}\")\nprint(\"  -1 to 0: Categories overlap (model not distinguishing)\")\nprint(\"  0 to 0.5: Some separation\")  \nprint(\"  0.5 to 1: Good separation (model learning distinct patterns)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a27pjyl7dpr",
   "source": "# Pattern Similarity - are same-category trades more similar internally?\nsim_matrix = cosine_similarity(hidden_states)\n\n# Calculate average similarity within and across categories\ndef avg_similarity(sim_matrix, categories, cat1, cat2):\n    idx1 = [i for i, c in enumerate(categories) if c == cat1]\n    idx2 = [i for i, c in enumerate(categories) if c == cat2]\n    if not idx1 or not idx2:\n        return 0\n    sub_matrix = sim_matrix[np.ix_(idx1, idx2)]\n    return sub_matrix.mean()\n\n# Within-category similarity (should be HIGH)\nsmall_small = avg_similarity(sim_matrix, categories, 'small', 'small')\nmid_mid = avg_similarity(sim_matrix, categories, 'mid', 'mid')\nbig_big = avg_similarity(sim_matrix, categories, 'big', 'big')\n\n# Cross-category similarity (should be LOWER)\nsmall_mid = avg_similarity(sim_matrix, categories, 'small', 'mid')\nsmall_big = avg_similarity(sim_matrix, categories, 'small', 'big')\nmid_big = avg_similarity(sim_matrix, categories, 'mid', 'big')\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Bar chart\nax = axes[0]\nlabels = ['Small-Small', 'Mid-Mid', 'Big-Big', 'Small-Mid', 'Small-Big', 'Mid-Big']\nvalues = [small_small, mid_mid, big_big, small_mid, small_big, mid_big]\ncolors = ['blue', 'green', 'red', 'purple', 'purple', 'purple']\nbars = ax.bar(labels, values, color=colors)\nax.set_ylabel('Average Cosine Similarity')\nax.set_title('Pattern Similarity\\n(Same-category should be higher than cross-category)')\nax.axhline(y=np.mean([small_mid, small_big, mid_big]), color='gray', linestyle='--', alpha=0.5, label='Avg cross')\nplt.xticks(rotation=45, ha='right')\nfor bar, val in zip(bars, values):\n    ax.text(bar.get_x() + bar.get_width()/2, val + 0.01, f'{val:.2f}', ha='center', fontsize=9)\n\n# Heatmap\nax = axes[1]\nsim_avg = np.array([\n    [small_small, small_mid, small_big],\n    [small_mid, mid_mid, mid_big],\n    [small_big, mid_big, big_big]\n])\nim = ax.imshow(sim_avg, cmap='viridis', vmin=0, vmax=1)\nax.set_xticks([0, 1, 2])\nax.set_yticks([0, 1, 2])\nax.set_xticklabels(['Small', 'Mid', 'Big'])\nax.set_yticklabels(['Small', 'Mid', 'Big'])\nax.set_title('Category Similarity Matrix')\nfor i in range(3):\n    for j in range(3):\n        ax.text(j, i, f'{sim_avg[i,j]:.2f}', ha='center', va='center', color='white', fontsize=12)\nplt.colorbar(im, ax=ax)\n\nplt.tight_layout()\nplt.show()\n\n# Interpretation\nwithin_avg = np.mean([small_small, mid_mid, big_big])\ncross_avg = np.mean([small_mid, small_big, mid_big])\nprint(f\"\\nWithin-category avg similarity: {within_avg:.3f}\")\nprint(f\"Cross-category avg similarity: {cross_avg:.3f}\")\nprint(f\"Difference: {within_avg - cross_avg:.3f}\")\nif within_avg > cross_avg + 0.05:\n    print(\"✓ Model is learning category-specific patterns\")\nelse:\n    print(\"✗ Model is NOT distinguishing categories well internally\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# V2 Analysis - Prediction quality\n\nmodel.eval()\n\n# Analyze P(liquidation) predictions\nliq_pred_vs_actual = {'true_pos': 0, 'true_neg': 0, 'false_pos': 0, 'false_neg': 0}\nsize_chosen_hist = {i: 0 for i in range(NUM_SIZES)}\nsize_optimal_hist = {i: 0 for i in range(NUM_SIZES)}\n\nwith torch.no_grad():\n    for candles, equity, targets, liq_targets, return_targets in val_loader:\n        # Move data to GPU\n        candles = candles.to(device)\n        equity = equity.to(device)\n        liq_targets = liq_targets.to(device)\n        return_targets = return_targets.to(device)\n        \n        p_liq, e_return = model(candles, equity)\n        chosen_idx = model.pick_size(candles, equity, LIQ_THRESHOLD)\n        \n        # Analyze liquidation predictions\n        pred_liq = (p_liq > 0.5).float()\n        \n        tp = ((pred_liq == 1) & (liq_targets == 1)).sum().item()\n        tn = ((pred_liq == 0) & (liq_targets == 0)).sum().item()\n        fp = ((pred_liq == 1) & (liq_targets == 0)).sum().item()\n        fn = ((pred_liq == 0) & (liq_targets == 1)).sum().item()\n        \n        liq_pred_vs_actual['true_pos'] += tp\n        liq_pred_vs_actual['true_neg'] += tn\n        liq_pred_vs_actual['false_pos'] += fp\n        liq_pred_vs_actual['false_neg'] += fn\n        \n        # Size distribution\n        for idx in chosen_idx:\n            size_chosen_hist[idx.item()] += 1\n        for idx in targets:\n            size_optimal_hist[idx.item()] += 1\n\n# Liquidation prediction stats\ntotal_liq_preds = sum(liq_pred_vs_actual.values())\nprint(\"Liquidation Prediction Quality (across all 30 sizes):\")\nprint(f\"  True Positive:  {liq_pred_vs_actual['true_pos']:7d} ({liq_pred_vs_actual['true_pos']/total_liq_preds*100:5.1f}%)\")\nprint(f\"  True Negative:  {liq_pred_vs_actual['true_neg']:7d} ({liq_pred_vs_actual['true_neg']/total_liq_preds*100:5.1f}%)\")\nprint(f\"  False Positive: {liq_pred_vs_actual['false_pos']:7d} ({liq_pred_vs_actual['false_pos']/total_liq_preds*100:5.1f}%)\")\nprint(f\"  False Negative: {liq_pred_vs_actual['false_neg']:7d} ({liq_pred_vs_actual['false_neg']/total_liq_preds*100:5.1f}%) <- DANGEROUS\")\n\n# Safety metrics\nif liq_pred_vs_actual['true_pos'] + liq_pred_vs_actual['false_neg'] > 0:\n    recall = liq_pred_vs_actual['true_pos'] / (liq_pred_vs_actual['true_pos'] + liq_pred_vs_actual['false_neg'])\n    print(f\"\\n  Liquidation Recall: {recall*100:.1f}% (ability to detect actual liquidations)\")\nif liq_pred_vs_actual['true_pos'] + liq_pred_vs_actual['false_pos'] > 0:\n    precision = liq_pred_vs_actual['true_pos'] / (liq_pred_vs_actual['true_pos'] + liq_pred_vs_actual['false_pos'])\n    print(f\"  Liquidation Precision: {precision*100:.1f}% (accuracy when predicting liquidation)\")\n\n# Plot size distributions\nfig, ax = plt.subplots(figsize=(14, 5))\n\nx = np.arange(NUM_SIZES)\nwidth = 0.35\n\nchosen_counts = [size_chosen_hist[i] for i in range(NUM_SIZES)]\noptimal_counts = [size_optimal_hist[i] for i in range(NUM_SIZES)]\n\nax.bar(x - width/2, chosen_counts, width, label='Model Chosen', alpha=0.7)\nax.bar(x + width/2, optimal_counts, width, label='Optimal', alpha=0.7)\n\nax.set_xlabel('Size Index')\nax.set_ylabel('Count')\nax.set_title('Size Distribution: Model vs Optimal')\nax.set_xticks(x[::3])  # Every 3rd tick\nax.set_xticklabels([f\"{SIZES[i]:.2f}\" for i in x[::3]], rotation=45)\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Category distribution\ncat_chosen = {'small': 0, 'mid': 0, 'big': 0}\ncat_optimal = {'small': 0, 'mid': 0, 'big': 0}\nfor i in range(NUM_SIZES):\n    size = SIZES[i]\n    cat = get_category(size)\n    cat_chosen[cat] += size_chosen_hist[i]\n    cat_optimal[cat] += size_optimal_hist[i]\n\nprint(\"\\nCategory Distribution:\")\nprint(f\"  {'Category':<8} | {'Chosen':>8} | {'Optimal':>8}\")\nprint(f\"  {'-'*8} | {'-'*8} | {'-'*8}\")\nfor cat in ['small', 'mid', 'big']:\n    print(f\"  {cat:<8} | {cat_chosen[cat]:>8} | {cat_optimal[cat]:>8}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Save final checkpoint\nsave_checkpoint(model, optimizer, scheduler, EPOCHS, history, metrics, best_regret, f'final_epoch_{EPOCHS}.pt')\nprint(f\"Final checkpoint saved: final_epoch_{EPOCHS}.pt\")\n\n# List all V2 checkpoints\nprint(f\"\\nAll V2 checkpoints in {CHECKPOINT_DIR}:\")\nfor f in sorted(os.listdir(CHECKPOINT_DIR)):\n    if f.endswith('.pt'):\n        ckpt = torch.load(f\"{CHECKPOINT_DIR}/{f}\", map_location=device, weights_only=False)\n        epoch = ckpt.get('epoch', '?')\n        regret = ckpt.get('metrics', {}).get('regret', None)\n        liq_rate = ckpt.get('metrics', {}).get('liquidation_rate', None)\n        if regret is not None:\n            print(f\"  {f}: epoch {epoch}, regret {regret:.2f}%, liq {liq_rate:.1f}%\")\n        else:\n            print(f\"  {f}: epoch {epoch}\")\n\nprint(f\"\\nBest model saved as 'best.pt' with regret {best_regret:.2f}%\")\nprint(f\"To resume: set LOAD_FROM = 'best.pt'\")"
  },
  {
   "cell_type": "code",
   "id": "ppsohm89ss",
   "source": "# Test on random trades - V2 with detailed output\nprint(\"Sample V2 predictions:\")\nprint(f\"{'':>4} | {'Optimal':>8} | {'Chosen':>8} | {'P(liq)':>7} | {'E[ret]':>7} | {'Actual':>8} | {'Cat':>6}\")\nprint(\"-\" * 70)\n\nmodel.eval()\nfor i in range(10):\n    trade = random.choice(val_trades)\n    \n    # Prepare input\n    candles_norm = normalize_candles_v2(trade['candles'])\n    if candles_norm.shape[0] < max_len:\n        padding = np.zeros((max_len - candles_norm.shape[0], INPUT_FEATURES))\n        candles_norm = np.vstack([candles_norm, padding])\n    \n    candles_t = torch.FloatTensor(candles_norm).unsqueeze(0).to(device)\n    equity_t = torch.FloatTensor([[EQUITY / 1000]]).to(device)\n    \n    with torch.no_grad():\n        p_liq, e_return = model(candles_t, equity_t)\n        chosen_idx = model.pick_size(candles_t, equity_t, LIQ_THRESHOLD).item()\n    \n    optimal_idx = SIZES.index(trade['optimal_size'])\n    optimal_size = trade['optimal_size']\n    chosen_size = SIZES[chosen_idx]\n    \n    # Get actual outcome for chosen size\n    all_results = trade['all_results']\n    chosen_result = all_results[chosen_idx]\n    actual_ret = -70 if chosen_result['liquidated'] else chosen_result['return_pct']\n    \n    match = \"ok\" if get_category(chosen_size) == trade['category'] else \"MISS\"\n    \n    print(f\"{i+1:>4} | {optimal_size:>8.2f} | {chosen_size:>8.2f} | \"\n          f\"{p_liq[0, chosen_idx].item():>6.1%} | {e_return[0, chosen_idx].item():>6.1f}% | \"\n          f\"{actual_ret:>7.1f}% | {match:>6}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}