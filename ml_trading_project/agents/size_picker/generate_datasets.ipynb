{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-0",
   "source": [
    "# Dataset Generator for Size Picker\n",
    "\n",
    "Generate large datasets of synthetic trades with controlled distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-1",
   "source": "# Setup + Mount Google Drive\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom datetime import datetime\n\n# Mount Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nSAVE_DIR = '/content/drive/MyDrive/size_picker_data'\nos.makedirs(SAVE_DIR, exist_ok=True)\nprint(f\"Saving to: {SAVE_DIR}\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-2",
   "source": [
    "# Configuration\n",
    "SIZES = [round(0.15 * i, 2) for i in range(1, 31)]  # 0.15 to 4.50\n",
    "LOOKBACK = 24\n",
    "\n",
    "# Size categories\n",
    "SMALL_SIZES = [s for s in SIZES if s <= 1.5]      # 0.15 to 1.50\n",
    "MID_SIZES = [s for s in SIZES if 1.5 < s <= 3.0]  # 1.65 to 3.00\n",
    "BIG_SIZES = [s for s in SIZES if s > 3.0]         # 3.15 to 4.50\n",
    "\n",
    "print(f\"Small sizes ({len(SMALL_SIZES)}): {SMALL_SIZES[0]} to {SMALL_SIZES[-1]}\")\n",
    "print(f\"Mid sizes ({len(MID_SIZES)}): {MID_SIZES[0]} to {MID_SIZES[-1]}\")\n",
    "print(f\"Big sizes ({len(BIG_SIZES)}): {BIG_SIZES[0]} to {BIG_SIZES[-1]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-3",
   "source": [
    "# Broker simulation\n",
    "class BrokerSim:\n",
    "    def __init__(self, equity=1000, leverage=20, liquidation_pct=0.30,\n",
    "                 spread_pct=0.0006, overnight_cost_per_oz=1.0, min_size=0.15):\n",
    "        self.initial_equity = equity\n",
    "        self.leverage = leverage\n",
    "        self.liquidation_level = equity * liquidation_pct\n",
    "        self.spread_pct = spread_pct\n",
    "        self.overnight_cost = overnight_cost_per_oz\n",
    "        self.min_size = min_size\n",
    "        \n",
    "    def run_trade(self, candles, entry_idx, exit_idx, direction, size):\n",
    "        if size < self.min_size:\n",
    "            size = self.min_size\n",
    "            \n",
    "        entry_price = candles.iloc[entry_idx]['close']\n",
    "        if direction == 'Buy':\n",
    "            entry_price *= (1 + self.spread_pct)\n",
    "        else:\n",
    "            entry_price *= (1 - self.spread_pct)\n",
    "        \n",
    "        equity = self.initial_equity\n",
    "        liquidated = False\n",
    "        max_drawdown_pct = 0\n",
    "        total_overnight = 0\n",
    "        last_date = pd.to_datetime(candles.iloc[entry_idx]['datetime']).date()\n",
    "        max_equity = equity\n",
    "        \n",
    "        for i in range(entry_idx + 1, exit_idx + 1):\n",
    "            bar = candles.iloc[i]\n",
    "            current_date = pd.to_datetime(bar['datetime']).date()\n",
    "            \n",
    "            if current_date > last_date:\n",
    "                days = (current_date - last_date).days\n",
    "                overnight = days * self.overnight_cost * size\n",
    "                total_overnight += overnight\n",
    "                equity -= overnight\n",
    "                last_date = current_date\n",
    "            \n",
    "            if direction == 'Buy':\n",
    "                worst_price = bar['low']\n",
    "                unrealized_worst = (worst_price - entry_price) * size\n",
    "                unrealized = (bar['close'] - entry_price) * size\n",
    "            else:\n",
    "                worst_price = bar['high']\n",
    "                unrealized_worst = (entry_price - worst_price) * size\n",
    "                unrealized = (entry_price - bar['close']) * size\n",
    "            \n",
    "            equity_at_worst = self.initial_equity + unrealized_worst - total_overnight\n",
    "            if equity_at_worst <= self.liquidation_level:\n",
    "                liquidated = True\n",
    "                equity = self.liquidation_level\n",
    "                break\n",
    "            \n",
    "            equity = self.initial_equity + unrealized - total_overnight\n",
    "            max_equity = max(max_equity, equity)\n",
    "            dd = (max_equity - equity) / max_equity * 100 if max_equity > 0 else 0\n",
    "            max_drawdown_pct = max(max_drawdown_pct, dd)\n",
    "        \n",
    "        if not liquidated:\n",
    "            exit_price = candles.iloc[exit_idx]['close']\n",
    "            if direction == 'Buy':\n",
    "                exit_price *= (1 - self.spread_pct)\n",
    "                final_pnl = (exit_price - entry_price) * size\n",
    "            else:\n",
    "                exit_price *= (1 + self.spread_pct)\n",
    "                final_pnl = (entry_price - exit_price) * size\n",
    "            equity = self.initial_equity + final_pnl - total_overnight\n",
    "        \n",
    "        pnl = equity - self.initial_equity\n",
    "        return_pct = (pnl / self.initial_equity) * 100\n",
    "        \n",
    "        return {\n",
    "            'liquidated': liquidated,\n",
    "            'return_pct': return_pct,\n",
    "            'max_drawdown_pct': max_drawdown_pct,\n",
    "            'size': size\n",
    "        }\n",
    "\n",
    "broker = BrokerSim()\n",
    "print(\"Broker ready\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-4",
   "source": "# Candle generation\ndef generate_candles(n_candles, base_price=4400, volatility=0.02):\n    candles = []\n    price = base_price\n    \n    for _ in range(n_candles):\n        change = price * volatility * random.gauss(0, 1)\n        open_p = price\n        close_p = price + change\n        high_ext = abs(random.gauss(0, 1)) * price * volatility * 0.5\n        low_ext = abs(random.gauss(0, 1)) * price * volatility * 0.5\n        high_p = max(open_p, close_p) + high_ext\n        low_p = min(open_p, close_p) - low_ext\n        \n        candles.append({\n            'open': open_p,\n            'high': high_p,\n            'low': low_p,\n            'close': close_p\n        })\n        price = close_p\n    \n    return pd.DataFrame(candles)\n\ndef generate_trade(lookback=24, trade_length_range=(10, 50), volatility_range=(0.005, 0.03)):\n    volatility = random.uniform(*volatility_range)\n    trade_length = random.randint(*trade_length_range)\n    total_candles = lookback + trade_length\n    \n    candles = generate_candles(total_candles, volatility=volatility)\n    candles['datetime'] = pd.date_range('2025-01-01', periods=total_candles, freq='h')\n    \n    entry_idx = lookback\n    exit_idx = total_candles - 1\n    direction = random.choice(['Buy', 'Sell'])\n    \n    return {\n        'candles_df': candles,\n        'entry_idx': entry_idx,\n        'exit_idx': exit_idx,\n        'direction': direction,\n        'volatility': volatility\n    }\n\nprint(\"Candle generator ready\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-5",
   "source": [
    "# Optimal size finder\n",
    "def get_optimal_size(candles_df, entry_idx, exit_idx, direction, sizes, broker):\n",
    "    results = []\n",
    "    for size in sizes:\n",
    "        result = broker.run_trade(candles_df, entry_idx, exit_idx, direction, size)\n",
    "        result['size'] = size\n",
    "        results.append(result)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df['score'] = df.apply(lambda r: -10 if r['liquidated'] else r['return_pct'] * 0.5, axis=1)\n",
    "    \n",
    "    best_idx = df['score'].idxmax()\n",
    "    optimal_size = df.loc[best_idx, 'size']\n",
    "    optimal_score = df.loc[best_idx, 'score']\n",
    "    \n",
    "    # Categorize\n",
    "    if optimal_size <= 1.5:\n",
    "        category = 'small'\n",
    "    elif optimal_size <= 3.0:\n",
    "        category = 'mid'\n",
    "    else:\n",
    "        category = 'big'\n",
    "    \n",
    "    return {\n",
    "        'optimal_size': optimal_size,\n",
    "        'optimal_score': optimal_score,\n",
    "        'category': category,\n",
    "        'all_results': df\n",
    "    }\n",
    "\n",
    "print(\"Optimal size finder ready\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-6",
   "source": "# Dataset generator V2 - stores all_results for two-head model training\n\ndef generate_all_datasets_v2(target_per_category=100000):\n    \"\"\"\n    Generate trades and sort into small/mid/big as we go.\n    V2: Stores simulation results for ALL 30 sizes per trade (needed for two-head model).\n    Saves progress every 100 trades.\n    Resumes from existing progress files if they exist.\n    \"\"\"\n    # Load existing progress if available (use v2 suffix to avoid conflicts)\n    small_path = f'{SAVE_DIR}/small_v2_progress.pkl'\n    mid_path = f'{SAVE_DIR}/mid_v2_progress.pkl'\n    big_path = f'{SAVE_DIR}/big_v2_progress.pkl'\n\n    if os.path.exists(small_path):\n        with open(small_path, 'rb') as f:\n            small_trades = pickle.load(f)\n        print(f\"Loaded {len(small_trades)} existing small trades\")\n    else:\n        small_trades = []\n\n    if os.path.exists(mid_path):\n        with open(mid_path, 'rb') as f:\n            mid_trades = pickle.load(f)\n        print(f\"Loaded {len(mid_trades)} existing mid trades\")\n    else:\n        mid_trades = []\n\n    if os.path.exists(big_path):\n        with open(big_path, 'rb') as f:\n            big_trades = pickle.load(f)\n        print(f\"Loaded {len(big_trades)} existing big trades\")\n    else:\n        big_trades = []\n\n    total_generated = len(small_trades) + len(mid_trades) + len(big_trades)\n    last_save = total_generated\n\n    print(f\"Target: {target_per_category} trades per category\")\n    print(f\"Starting from: small={len(small_trades)}, mid={len(mid_trades)}, big={len(big_trades)}\")\n    print(\"Generating (V2 with all_results)...\")\n\n    while (len(small_trades) < target_per_category or\n           len(mid_trades) < target_per_category or\n           len(big_trades) < target_per_category):\n\n        trade_data = generate_trade()\n        optimal_info = get_optimal_size(\n            trade_data['candles_df'],\n            trade_data['entry_idx'],\n            trade_data['exit_idx'],\n            trade_data['direction'],\n            SIZES,\n            broker\n        )\n\n        candle_start = trade_data['entry_idx'] - LOOKBACK\n        trade_candles = trade_data['candles_df'].iloc[candle_start:trade_data['exit_idx']+1][['open', 'high', 'low', 'close']].values\n\n        # Convert all_results DataFrame to list of dicts for storage\n        all_results_list = optimal_info['all_results'][['size', 'liquidated', 'return_pct', 'max_drawdown_pct']].to_dict('records')\n\n        trade = {\n            'trade_id': total_generated + 1,\n            'direction': trade_data['direction'],\n            'candles': trade_candles,\n            'optimal_size': optimal_info['optimal_size'],\n            'optimal_score': optimal_info['optimal_score'],\n            'category': optimal_info['category'],\n            'volatility': trade_data['volatility'],\n            # V2: Store results for all 30 sizes\n            'all_results': all_results_list\n        }\n\n        # Sort into category (only if we still need it)\n        if optimal_info['category'] == 'small' and len(small_trades) < target_per_category:\n            small_trades.append(trade)\n        elif optimal_info['category'] == 'mid' and len(mid_trades) < target_per_category:\n            mid_trades.append(trade)\n        elif optimal_info['category'] == 'big' and len(big_trades) < target_per_category:\n            big_trades.append(trade)\n\n        total_generated += 1\n        total_saved = len(small_trades) + len(mid_trades) + len(big_trades)\n\n        # Save every 100 trades added\n        if total_saved - last_save >= 100:\n            last_save = total_saved\n            print(f\"Total: {total_generated} | small={len(small_trades)}, mid={len(mid_trades)}, big={len(big_trades)}\")\n\n            # Save progress\n            save_dataset(small_trades, 'small_v2_progress')\n            save_dataset(mid_trades, 'mid_v2_progress')\n            save_dataset(big_trades, 'big_v2_progress')\n\n    # Final save\n    save_dataset(small_trades, f'small_v2_{target_per_category//1000}k')\n    save_dataset(mid_trades, f'mid_v2_{target_per_category//1000}k')\n    save_dataset(big_trades, f'big_v2_{target_per_category//1000}k')\n\n    print(f\"\\nDone! Total generated: {total_generated}\")\n    print(f\"Small: {len(small_trades)}, Mid: {len(mid_trades)}, Big: {len(big_trades)}\")\n\n    return small_trades, mid_trades, big_trades\n\nprint(\"Generator ready - use generate_all_datasets_v2() for two-head model data\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-7",
   "source": "# Save/Load functions\ndef save_dataset(trades, filename):\n    filepath = f'{SAVE_DIR}/{filename}.pkl'\n    with open(filepath, 'wb') as f:\n        pickle.dump(trades, f)\n    print(f\"Saved {len(trades)} trades to {filepath}\")\n    \ndef load_dataset(filename):\n    filepath = f'{SAVE_DIR}/{filename}.pkl'\n    with open(filepath, 'rb') as f:\n        trades = pickle.load(f)\n    print(f\"Loaded {len(trades)} trades from {filepath}\")\n    return trades\n\ndef list_datasets():\n    files = [f for f in os.listdir(SAVE_DIR) if f.endswith('.pkl')]\n    print(\"Available datasets:\")\n    for f in files:\n        filepath = f'{SAVE_DIR}/{f}'\n        with open(filepath, 'rb') as file:\n            trades = pickle.load(file)\n        print(f\"  {f}: {len(trades)} trades\")\n    return files\n\nprint(\"Save/Load ready\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-8",
   "source": "---\n## Option 1: Convert Existing Datasets to V2 (Faster)\n\nIf you already have V1 datasets (balanced_100k, etc.), convert them to V2 by re-running broker simulation for all 30 sizes. This is much faster than regenerating from scratch."
  },
  {
   "cell_type": "code",
   "id": "0uwk6gkj17sq",
   "source": "# Convert V1 dataset to V2 (add all_results)\ndef convert_to_v2(trades, broker):\n    \"\"\"\n    Convert existing V1 trades to V2 by running broker simulation for all 30 sizes.\n    \n    The stored candles already contain everything we need:\n    - entry_idx = LOOKBACK (24)\n    - exit_idx = len(candles) - 1\n    \"\"\"\n    v2_trades = []\n    \n    for i, trade in enumerate(trades):\n        # Reconstruct what broker needs\n        candles = trade['candles']\n        direction = trade['direction']\n        \n        # The stored candles start at (original_entry - LOOKBACK) and end at exit\n        # So entry_idx in this array is LOOKBACK, exit_idx is len-1\n        entry_idx = LOOKBACK\n        exit_idx = len(candles) - 1\n        \n        # Create DataFrame with datetime for broker (it needs datetime for overnight calc)\n        candles_df = pd.DataFrame(candles, columns=['open', 'high', 'low', 'close'])\n        candles_df['datetime'] = pd.date_range('2025-01-01', periods=len(candles), freq='h')\n        \n        # Run simulation for all 30 sizes\n        results = []\n        for size in SIZES:\n            result = broker.run_trade(candles_df, entry_idx, exit_idx, direction, size)\n            result['size'] = size\n            results.append(result)\n        \n        results_df = pd.DataFrame(results)\n        all_results_list = results_df[['size', 'liquidated', 'return_pct', 'max_drawdown_pct']].to_dict('records')\n        \n        # Create V2 trade (copy existing + add all_results)\n        v2_trade = trade.copy()\n        v2_trade['all_results'] = all_results_list\n        v2_trades.append(v2_trade)\n        \n        if (i + 1) % 10000 == 0:\n            print(f\"  Converted {i + 1}/{len(trades)} trades\")\n    \n    return v2_trades\n\ndef convert_dataset_to_v2(input_name, output_name):\n    \"\"\"Load a V1 dataset, convert to V2, and save.\"\"\"\n    print(f\"Loading {input_name}...\")\n    trades = load_dataset(input_name)\n    \n    if 'all_results' in trades[0]:\n        print(f\"  Already V2 format! Skipping.\")\n        return trades\n    \n    print(f\"Converting {len(trades)} trades to V2...\")\n    v2_trades = convert_to_v2(trades, broker)\n    \n    print(f\"Saving as {output_name}...\")\n    save_dataset(v2_trades, output_name)\n    \n    return v2_trades\n\nprint(\"Conversion functions ready\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "vnfbqxi6iwt",
   "source": "# Convert existing balanced_100k to V2\n# This will run broker simulation for all 30 sizes on each trade\n\nbalanced_v2 = convert_dataset_to_v2('balanced_100k', 'balanced_v2_100k')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3xhjroc0c9l",
   "source": "# Optional: Convert reinforcement_200k to V2\n# reinforcement_v2 = convert_dataset_to_v2('reinforcement_200k', 'reinforcement_v2_200k')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "jm9lzli8l4q",
   "source": "---\n## Option 2: Generate Fresh V2 Datasets\n\nGenerate completely new datasets with all_results included from the start.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-9",
   "source": "# Generate 100k of each category (V2 with all_results)\n# This runs until all three have 100k, saving progress every 100 trades\nsmall_trades, mid_trades, big_trades = generate_all_datasets_v2(100000)",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-10",
   "source": "# Create balanced V2 dataset (equal parts small, mid, big)\nimport random\n\nbalanced = small_trades[:33333] + mid_trades[:33333] + big_trades[:33334]\nrandom.shuffle(balanced)\nsave_dataset(balanced, 'balanced_v2_100k')\nprint(f\"Created balanced V2 dataset: {len(balanced)} trades\")\nprint(f\"  Sample trade has 'all_results': {'all_results' in balanced[0]}\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-11",
   "source": "# Create reinforcement V2 dataset (remaining trades after balanced)\nreinforcement = small_trades[33333:] + mid_trades[33333:] + big_trades[33334:]\nrandom.shuffle(reinforcement)\nsave_dataset(reinforcement, 'reinforcement_v2_200k')\nprint(f\"Created reinforcement V2 dataset: {len(reinforcement)} trades\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-12",
   "source": "# List all datasets\nlist_datasets()",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-13",
   "source": [
    "# List all generated datasets\n",
    "list_datasets()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-14",
   "source": [
    "---\n",
    "## Combine Datasets\n",
    "\n",
    "Combine datasets for training curriculum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-15",
   "source": [
    "# Example: Create balanced training set\n",
    "# Load and combine\n",
    "# small = load_dataset('small_100k')\n",
    "# mid = load_dataset('mid_100k')\n",
    "# big = load_dataset('big_100k')\n",
    "# \n",
    "# balanced = small[:33333] + mid[:33333] + big[:33334]\n",
    "# random.shuffle(balanced)\n",
    "# save_dataset(balanced, 'balanced_100k')"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}